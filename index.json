[{"categories":["community","UnconventionalContributors"],"content":"We have many diverse contributors in opensource that help upstream communities in unconventional ways, which doesn’t require any coding or development skills. Starting with a non-code contribution can help anyone overcome the sense of failure and not being good enough, and it can also serve as a springboard for our open source adventure. This interview series aims to highlight some non-code open source contributions that anyone can make right now to get started contributing. For this month’s edition, we talked about maintaing projects, translating kubernetes articles and involving more people into open source ecosystem with Pradumna Saraf. Thank you for taking some time out and interacting with us, Pradumna. Can you introduce yourself and share your journey so far? Hello, my name is Pradumna Saraf, and I am a devops and Go developer from India. My expertise includes working with DevOps tools such as Kubernetes, Docker, and other CNCF-based projects. I also enjoy building CLI and automation tools/projects. In addition to my technical skills, I am a strong advocate for open source literacy and inclusivity. I believe that open source is for everyone, and I strive to involve more people in the ecosystem by creating content on Twitter and LinkedIn, writing blogs, and producing videos. Currently, I am in my third year of college and have done an internships at open source companies where I helped uplift their organizations. Going forward this year, I want to choose to work in devrel and focus more on the community side of things . Going forward this year, I want to enter the field of developer relations (devrel) to become more involved in the community side of things. What communities have you contributed to? I have contributed to over 100 repositories, but I believe that people often have a limited view of what open source contributions entail. Open source contributions can take many forms, such as advocacy, writing, and blogging. Open source is a vast ecosystem, and my contributions extend beyond just pull requests. While I have contributed to various projects, including those of Google and Microsoft, my focus is currently on the Kubernetes project. DevOps and open source are a great fit, and many DevOps tools are open source, so I find Kubernetes and Docker to be particularly relevant. I am a GitHub member of the Kubernetes and Kubernetes SIGs Org. I contribute both through code and content creation. Overall, I engage in multiple types of contributions to support the open source community. How did your contribution journey start? My journey started in 2021 during Hacktoberfest, a month-long open source fest for FOSS organizations. I began contributing to open source by participating in Hacktoberfest, but I truly discovered the benefits of contributing through EddieHub. The community members were willing to help me in my contribution journey, even though I didn’t know much about coding at the time. The inclusivity and international exposure of the community were particularly appealing to me, as I was able to connect with people from all over the world. In open source collaboration, anyone can contribute, collaborate, and connect with others without any barriers or gatekeeping. Everything is transparent, and people come together to discuss issues, features, and ideas. As I started contributing to repositories, I realized that it was important to know a subject well before writing or talking about it. This prevented me from spreading misinformation to others. I became familiar with tools like Git, GitHub, and began writing blog posts on topics that I was learning on a daily basis, such as how to use GitHub Actions and automate tasks. My LinkedIn and Twitter posts reflect my daily learnings, and I share my insights on various topics. For example, I recently learned about ArgoCD, and I shared my thoughts about it on LinkedIn and Twitter. This is how I contribute to open source by sharing my knowledge with others. In what ways have you","date":"2023-02-22","objectID":"/contributor_7/:0:0","tags":["blogs","community","eddiehub"],"title":"Contributor Blog #7: Interview with Pradumna Saraf","uri":"/contributor_7/"},{"categories":["How-to","VisitingAuthors","technology"],"content":"Domain Name System With over 8 billion cellphones worldwide (not including other devices), it is hard to correctly recall everyone’s names and contact information. Because of this, the Domain Name System—or more precisely, the Phone Directory for IP Addresses—was introduced. However, due to the memory limitations of the human brain, we are unable to recall all of these billions of IP addresses. In order to make it easier to remember names, we began referring to these IP addresses by names, which are simply the website names that we type into the address bar of a browser. The task of determining which name is associated with which IP address is handled by DNS. To put it simply, it maps the Name to the available IP addresses. Everyone who uses a digital device and the internet uses DNS, or domain name system, without knowing(not that they need to) how it enables them to find the exact content they are seeking online. Take an example that you have no idea what a phone book is. How will you remember all of the contact details of everyone you meet and try to stay in touch with? This is essential if you want to be able to reach the specific person you want to. Indeed, you can’t do it properly, which is how the idea of a phone directory emerged. Similar to this, the digital gadgets we use every day, such as smartphones, routers, speakers, and other items that may connect to the internet, have their own naming conventions. These names are referred to as IP addresses. Coming to the technical part of how it works and get us the IP address, where it gets the IP address from? Let us first know the components and jargons that are involved in the above process. ","date":"2022-11-26","objectID":"/dns/:1:0","tags":["How-to","technology","dns","GauravKale","VisitingAuthors"],"title":"What is DNS?","uri":"/dns/"},{"categories":["How-to","VisitingAuthors","technology"],"content":"Components and jargons DNS recursor The recursor might be likened to a librarian who is asked to search a library for a certain book. A server called the DNS recursor is made to take requests from client devices using software like web browsers. In most cases, the recursor is then in charge of sending further queries to respond to the client’s DNS query. The Root nameserver It is the first place where human readable host names are converted (resolved) into IP addresses. It can be compared to an index in a library that directs users to various book racks; often, it acts as a pointer to other, more precise locations. The top-level domain server (TLD) Can be compared to a particular shelf of books in a library. The final part of a hostname is hosted by this nameserver, which is the following stage in the process of locating an IP address (in the case of example.com, the TLD server is “com”). Authoritative nameserver This last nameserver can be compared to a dictionary on a shelf of books, where a name’s definition can be found. The final stop in the nameserver query is the authoritative nameserver. The IP address for the requested hostname will be returned to the DNS Recursor (the librarian) who initiated the initial request if the authoritative name server has access to the requested record. ","date":"2022-11-26","objectID":"/dns/:2:0","tags":["How-to","technology","dns","GauravKale","VisitingAuthors"],"title":"What is DNS?","uri":"/dns/"},{"categories":["How-to","VisitingAuthors","technology"],"content":"Step that make up a DNS lookup: A user types “example.com” into their web browser, which sends the query out over the Internet and into a DNS recursive resolver. A DNS root nameserver is then questioned by the resolver (.). The Top Level Domain (TLD) DNS server address (such as.com or.net), which holds the data for its domains, is then returned to the resolver by the root server. Our request is directed at the.com TLD while looking for example.com. After that, the resolver sends an inquiry to the.com TLD. The TLD server then replies with example.com’s nameserver’s IP address. The recursive resolver then queries the nameserver for the domain. The nameserver then provides the resolver with the IP address for example.com. After that, the DNS resolver replies to the web browser with the IP address of the domain that was originally requested. And after this, the browser is able to get the data for the website and display it to the user. ","date":"2022-11-26","objectID":"/dns/:3:0","tags":["How-to","technology","dns","GauravKale","VisitingAuthors"],"title":"What is DNS?","uri":"/dns/"},{"categories":["How-to","VisitingAuthors","technology"],"content":"Types of Queries: 1. Recursive query In a recursive query, a DNS client expects a DNS server (usually a DNS recursive resolver) to respond with either the requested resource record or, in the event that the resolver is unable to locate it, an error message. 2. Iterative query In this case, the DNS client will let the DNS server give the best response it is able to. A referral to a DNS server authoritative for a lower level of the domain namespace will be returned if the queried DNS server does not have a match for the query name. The recommendation address will then be contacted by the DNS client. Up the query chain, this process is repeated with other DNS servers until an error or timeout happens. Hope this article was helpful. Happy learning :) Gaurav Kale a tech enthusiast and a learner. He mostly deals with security and internet technologies and takes part in activities that can aid in promoting a better understanding of technology. In his free time, he enjoys reading books and watching movies. He can be reached out on LinkedIn, Twitter or via email. ","date":"2022-11-26","objectID":"/dns/:4:0","tags":["How-to","technology","dns","GauravKale","VisitingAuthors"],"title":"What is DNS?","uri":"/dns/"},{"categories":["community","UnconventionalContributors"],"content":"We have many diverse contributors in opensource that help upstream communities in unconventional ways, which doesn’t require any coding or development skills. Starting with a non-code contribution can help anyone overcome the sense of failure and not being good enough, and it can also serve as a springboard for our open source adventure. This interview series aims to highlight some non-code open source contributions that anyone can make right now to get started contributing. For this month’s edition, we talked about finding suitable projects to contribute, testing, documentation and speaking at conferences with Manaswini Das. Thank you for taking some time out and interacting with us, Manaswini. Can you introduce yourself and share your journey so far? I am Manaswini Das, and I have been working as a software engineer at Red Hat for over three years. I hail from Bhubaneswar, India. I wasn’t an engineer by choice but by a fortunate stroke of luck. I just knew I loved mathematics and programming(whatever I could learn in school) after I finished school. I decided to pursue a Bachelor’s degree in Computer Science, and that’s how this journey started. In 2019, I joined Red Hat as an intern since I wanted to align my professional career with my love for open source. I have been working here for three years now. I still volunteer/speak at/organise meetups/conferences. What communities have you contributed to? As an intern, I, with the help of a few others, established a unified architecture for single-sourcing documentation between community projects and products. After that, I started contributing to DashBuilder, a part of RHPAM/DM, as a part of the Middleware India team. I have contributed to other open-source communities like JBoss, Apache, Quarkus, Tern, Fossasia, Appwrite, GirlScript Foundation, etc. How did your contribution journey start? I got introduced to Linux when I joined the tech society of my college when I was a sophomore. I decided to participate in RGSoC and contributed to the HospitalRun project, and that’s how I was introduced to open-source programs. Initially, I started playing around with Android programming and then moved to cross-platform app development using Ionic and Cordova. Then, I slowly moved on to learning Python and JavaScript and Git. I noticed that all of this was open-source too. I decided to dive deeper and made my first open-source contribution during Hacktoberfest 2017. I learnt that contributing to open-source will help me gain experience without getting an internship because that’s what most students face. If you don’t have experience, you don’t get an internship and vice-versa. I made it a habit to explore open-source projects and contribute in whatever way I could. While zeroing in on projects to contribute to, I usually go through the website and try to understand the vision of the project. Then I try out the demos and if it piques my interest, I go through the documentation and start setting up the project and then start contributing. I try to look for projects that are for making people’s lives easier or come in the category for “Tech for good”. That’s how I got introduced to the Outreachy program. I asked my female seniors, and since nobody from our college had participated in this program before, they were skeptical about it. I decided to try, nevertheless. I got accepted in the Mar-May 2018 cohort(1st ever Outreachy intern from our college) under the Open Humans Foundation. This project opened doors to new opportunities for me and boosted my confidence. I got a chance to deliver a talk at DjangoCon Europe 2019 about my Outreachy project(My project required me to add GitHub and Twitter data sources to the Open Humans platform. I was new to Django but not new to Python so I gave it a try). I was the only Indian speaker and a student at that time too. But I was surprised to see 400+ attendees, including senior engineers and managers, patiently listening to my talk. That’s when I realise","date":"2022-11-16","objectID":"/contributorblog_6/:0:0","tags":["blogs","community","outreachy","python","django","p5.js"],"title":"Contributor Blog #6: Interview with Manaswini Das","uri":"/contributorblog_6/"},{"categories":["community","UnconventionalContributors"],"content":"Resources Conference Talks | https://github.com/manaswinidas/conferences_talks/ Manaswini’s articles | https://manaswinidas.medium.com/ Good First Issues | https://goodfirstissue.dev/ Playlist of Talks | https://www.youtube.com/playlist?list=PLAzCkRsKko5vp1pQZqrcSIP7SWAPlfeRa Mentor Without Borders | https://www.mentorswithoutborders.net/ Outreachy | https://www.outreachy.org/ Manaswini Das is from Bhubaneswar and currently working with the Red Hat Middleware team in Bangalore, India. She has been an active open-source user and contributor since 2017. She has participated both as a mentee and a mentor in various open-source programs including GCI, GSSoC, and RHOSC. She is an Outreachy alumnus and a Processing Foundation fellow. She loves sharing her knowledge at events and sketching. She can be reached out on LinkedIn, Twitter or via email This blog post is part of the Nov edition of UnconventionalContributors, our monthly interview series about different ways to contribute to opensource. If you like this article, check out the stories of our other contributors and stay tuned for our upcoming editions. Have a story to share? We’d be delighted to get in touch and discuss sharing your story. We are also open to suggestions for new content that will foster the community’s growth. We’ll see you all in the next one. Till then, Happy contributing!👋🏻 ","date":"2022-11-16","objectID":"/contributorblog_6/:1:0","tags":["blogs","community","outreachy","python","django","p5.js"],"title":"Contributor Blog #6: Interview with Manaswini Das","uri":"/contributorblog_6/"},{"categories":["How-to","VisitingAuthors","technology"],"content":"While exploring Jenkins, I came across a usecase wherein I wanted to make use of Jenkins slave to build some of my jobs. So after exploring I found out a way wherein we can deploy Jenkins slave as containers. Once the job is complete, the containers will be destroyed, thus releasing the resources. This removes the dependency to have extra VM’s or need for more storage to accomodate the jobs. This article is based on configurations tried and tested on MAC. For other environments, you need to make appropriate changes on Step B. Assumptions : You have Jenkins Master already installed on docker as a container. If not you can use the following command to launch the jenkins container. docker run --name jenkins -p 8080:8080 -p 50000:50000 jenkins/jenkins Note : Complete the installation with all the plugins. Follow below Steps to setup jenkins build agents : A. You need to expose the docker api, so that the jenkins can launch the jenkins build agent to complete it’s jobs. You can use the following command to expose the docker API to be used, docker run -d --restart=always -p 127.0.0.1:2376:2375 -v /var/run/docker.sock:/var/run/docker.sock alpine/socat tcp-listen:2375,fork,reuseaddr unix-connect:/var/run/docker.sock B. Once the above command is ran, you can check if the docker api is actually exposed to be used. Login to jenkins container and run the below command, docker exec \u003ccontainer-name\u003e curl http://docker.for.mac.localhost:2376/v1.41/info C. Once you get an output, you need to install a plugin on Jenkins called Docker. D. Next you need to configure the jenkins build agent ( slave ) templates. Navigate to Manage Jenkins –\u003e Manage Nodes and Clouds –\u003e Configure Clouds You will see following screen, Note : You will only see Add a new cloud option. E. Once you navigated to above location, click on Add a new cloud –\u003e Select Docker –\u003e Then click on Docker Cloud details and configure the following settings, Docker Host URI : tcp://docker.for.mac.host.internal:2376 Enabled : Select the option F. Now you can see the Test Connection option, please click on it and you should see the version of docker installed, If you got the ouput on step B, then you shouldn’t face any issues. G. Next we need to configure the Docker Agent templates, this template will be used by Jenkins to launch the Jenkins build agent container. Label : If you need to schedule a particular job to run on particular jenkins slave node. Enabled : Select the option, so that jenkins can auto launch the containers when it needs to run the job Name : Can be anything as per your preference Docker Image : jenkins/agent –\u003e Preferred image for launching the container. There is another image called jenkins/slave but it has some issues, so it’s best to go with jenkins/agent Registry Authentication : Use the creds you use to login to registry. The creds you use to login to hub.docker.com Remote File System Root : Basically the path where you job would run Connect method : Attach docker container Done this would setup the agent template. Once above settings are done, you are good to run the job using jenkin build agent on docker container. I hope you liked the article. Cheers! Nihal Shah is a Devops engineer who loves to explore new tools and automate different processes for ease of use. He can be reached out on LinkedIn, Twitter or via email. ","date":"2022-11-12","objectID":"/jenkins_docker_master_slave/:0:0","tags":["How-to","technology","jenkins","docker","NihalShah","VisitingAuthors"],"title":"Deploying Jenkins slave using docker containers","uri":"/jenkins_docker_master_slave/"},{"categories":["community","UnconventionalContributors"],"content":"We have many diverse contributors in opensource that help upstream communities in unconventional ways, which doesn’t require any coding or development skills. Starting with a non-code contribution can help anyone overcome the sense of failure and not being good enough, and it can also serve as a springboard for our open source adventure. This interview series aims to highlight some non-code open source contributions that anyone can make right now to get started contributing. For this month’s edition, we talked about mentoring, translating articles and volunteering at freeCodeCamp with Gláucia Esppenchutz. Thank you for taking some time out and interacting with us, Glaucia. Can you introduce yourself and share your journey so far? Hi, my name is Gláucia, I am a Brazilian and living in Porto, Portugal. I have been working as a software engineer for almost 10 years and being working as a data engineer for 6 years. I have not graduated in Computer Science or any other tech school, actually I have a biomedical degree but the feeling of belonging in this profession started to fall apart in the second year of the graduation. I decided to shift my career after I met my husband at the end of my graduation. He worked already as a software engineer and I found the profession really amazing. He also gave me a lot of support in transitioning my career. To enter the tech area I started studying by myself watching videos on Coursera, Udemy, Youtube and so on. The development community helped me a lot by mentoring me and guiding me. Python Brasil, a community for Brazilian developers who wants to code python, mentored me. When I was in the beginning of my career I had no idea where to start looking or where to begin studying from, so people from python Brasil helped me to get started. I started by studying the python principles, the best practices, what documentation should I start looking for some practical examples, some sites where I could practice code like codility, hacker rank. Another community that mentored me was Data boot camp. It’s like a small company that teaches data fields. This community helped me to understand where to start in the data world. Until this point I already learned python but now I needed to learn a bit more about SQL and then I needed to know a little bit more about statistics if I want to work as a data scientist. If not, I want to work as a data engineer. So I started looking for Hadoop and Spark and how to use it and they gave me very practical examples on that and that’s how the mentoring happened. They were guiding me on what should be the next steps. I can’t say the path to become a data engineer was easy, but a lot of people supported me directly and indirectly, like making docs and free courses available, promoting live talks and events to tell more about their careers, and making community tech events like meetups. There were a few particular live talks that helped, for example Jeferson Fernando conducts a live talk almost every week and some of his talks helped me alot. There was another speaker, Gomez who helped a lot, however, he’s not doing any live talks anymore. What communities have you contributed to? After I got my first job in 2014 as a Python developer, I decided to help other women get into the tech field. That decision was primarily based on the experiences I already had at my first job; no women at the office working as a developer, and the invalidation I had from some other colleagues for being a woman working on a “men” job. With another woman named Amanda, who is now currently living in Ireland, we spoke with the PyLadies organization and founded Brazil’s first dedicated core in Rio de Janeiro. The first core in PyLadies was in San Francisco. We contacted the girls there and asked what was expected in the meetings and is there any content available there for girls in Brazil. They suggested we start with a small group in Rio de Janeiro and call it PyLadies Rio. We then promoted talks, me","date":"2022-10-16","objectID":"/contributorblog_5/:0:0","tags":["blogs","community","freeCodeCamp","python"],"title":"Contributor Blog #5: Interview with Gláucia Esppenchutz","uri":"/contributorblog_5/"},{"categories":["community","UnconventionalContributors"],"content":"We have many diverse contributors in opensource that help upstream communities in unconventional ways, which doesn’t require any coding or development skills. Starting with a non-code contribution can help anyone overcome the sense of failure and not being good enough, and it can also serve as a springboard for our open source adventure. This interview series aims to highlight some non-code open source contributions that anyone can make right now to get started contributing. For this month’s edition, we talked about Ambassadorships, participating in Forums and writing Tech Blogs and with Geoff Burke. Thank you for taking some time out and interacting with us, Geoff. Can you introduce yourself and share your journey so far? My name is Geoff Burke. I’ve been in I.T for 20 years and I’ve been mainly dealing with data protection for the last 13 years, originally starting with protecting physical machines then virtual machines and recently have started working on containers and Kubernetes. While exploring Kubernetes, I thought of setting it up over a weekend, but that didn’t happen obviously because I had assumed, with all my experience, it would be easy. But it wasn’t. I quickly just leveraged Docker Swarm because it was a lot more simple, only to realize that the complexity of kubernetes also created enormous flexibility and enormous capabilities. I ended up doing all the related certification exams [CNA, CKAD, CKA and CKS] which were very difficult and challenging. Right now, I’m trying to move my focus totally towards kubernetes which is a bit of a challenge. I also believe this might be a challenge for a lot of people out there, hence, I’m very interested in helping people out because I realize that there are going to be a lot of people who are going to have to take these steps, like I did. Especially those who have already established themselves as virtual machines administrators and now have to make this big change of stepping into container orchestration. I have also worked with two major Canadian service providers. One provided backup and data protection and the second one provided an enormous amount of hosting as well, not just in Canada and States, but all over the world. I then became a Calico Ambassador because when I first started working on Kubernetes, Calico was the first CNI I used and ever since then I’ve never looked back. I remember I had some issues with flannel which got me annoyed and then I just put in Calico and it worked. So that’s where I am at right now. Was Kubernetes your first experience working with an open source project? I got introduced to open source products through various linux distros. I started playing around with Linux back in the late 90s. I remember I bought a Red Hat book, can’t remember the name, but it was a Red Hat book with a CD. I was in a different location and they had pulse dial-up. I was trying to get the modem to dial-up. With Windows it was easy, just click on the next button and you are done. With Linux, it was very difficult, but when it got working I was so happy, plus it was a pulse dial-up. I was also involved with Slackware and Debian. I’ve known quite a bit of Linux which helped with kubernetes but it didn’t mean it was going to be easy. I’ve never been a developer so I haven’t done any coding, I’ve done some bash scripting but very minimal, the basic stuff. What product(s)have you contributed to? I am mainly engaged with and contributing to communities more than the product itself. I have been writing lots of posts, beginner posts for people to learn. I also run a YouTube show. Alara Ozturk from Calico was my recent guest. Even though I’ve taken all the exams relevant to my roles, I still consider myself to be in a learning mode because I don’t have enough production experience. I think the big chunk of learning is the theory, which is covered while preparing for the exams, but then a huge chunk is the experience. Both theory and experience is necessary and if you don’","date":"2022-09-21","objectID":"/contributorblog_4/:0:0","tags":["blogs","community","Calico Open Source","Calico Big Cat"],"title":"Contributor Blog #4: Interview with Geoff Burke","uri":"/contributorblog_4/"},{"categories":["community","UnconventionalContributors"],"content":"Resources Geoff’s Blogsite | http://geoffburkeblog.com/ 90DaysofDevOps | https://github.com/MichaelCade/90DaysOfDevOps The power of technical communities Calico study case | https://youtu.be/akxqyLAh6G0 The Kube Vanguard | https://www.youtube.com/channel/UC4mrp3YgPxtIT3xdPi918_w/videos Calico Big Cats | https://www.tigera.io/blog/introducing-our-exciting-new-ambassador-program-calico-big-cats/ Geoff Burke is a Senior Cloud Solutions Architect at Tsunati with over 20 years of experience in IT and 13 years working with Data Protection. He has worked with top Canadian Service Providers leveraging Veeam Cloud Connect. Recently he has been become intensely focused on Kubernetes. Certifications include VMCA2022 VMCE2021, KCNA, CKS, CKAD, CKA. In his spare time, he likes to spend time with family, cycle, camp, see films, and watch documentaries. He also spends a lot of his free time in learning technology. He can be reached out on LinkedIn, Twitter or via email This blog post is part of the Sept edition of UnconventionalContributors, our monthly interview series about different ways to contribute to opensource. Don’t forget to check out the next one featuring Gláucia Esppenchutz, Staff Data Engineer at Cloudera. Have a story to share? We’d be delighted to get in touch and discuss sharing your story. We are also open to suggestions for new content that will foster the community’s growth. We’ll see you all in the next one. Till then, Happy contributing!👋🏻 ","date":"2022-09-21","objectID":"/contributorblog_4/:1:0","tags":["blogs","community","Calico Open Source","Calico Big Cat"],"title":"Contributor Blog #4: Interview with Geoff Burke","uri":"/contributorblog_4/"},{"categories":["How-to","technology"],"content":"In this article, I will share my experience in becoming a Certified Calico Operator: Level 1. Most of the information is available on the course portal and Calico’s webpages. I’ve made an effort to compile all the essential details about the course and exam in one location, and I hope it will be helpful to you. ","date":"2022-08-26","objectID":"/certified-calico-operator/:0:0","tags":["calico","Project Calico","networking","Kubernetes","CloudNative","How-to"],"title":"Certified Calico Operator: Level1","uri":"/certified-calico-operator/"},{"categories":["How-to","technology"],"content":"What is Project Calico? Project Calico is an open-source project with an active development and user community. Calico Open Source was born out of this project and has grown to be the most widely adopted solution for container networking and security, powering 2M+ nodes daily across 166 countries. ","date":"2022-08-26","objectID":"/certified-calico-operator/:1:0","tags":["calico","Project Calico","networking","Kubernetes","CloudNative","How-to"],"title":"Certified Calico Operator: Level1","uri":"/certified-calico-operator/"},{"categories":["How-to","technology"],"content":"What does the course cover? This course will cover how Kubernetes networking works, how to configure and manage a Calico network, and how to secure your Kubernetes cluster. Introduction to Kubernetes Networking Installing Calico Everything you need to know about Network Policy, including: Introduction to Network Policy Best Practices for Network Policy Managing Trust Across Teams Network Policy for Hosts and NodePorts Everything you need to know about network connectivity, including: Pod Connectivity Fundamentals Calico’s Next Generation eBPF Dataplane Encrypting Data in Transit Fun with IP Address Management Peering with BGP Everything you need to know about Kubernetes Services, including: Introduction to Kubernetes Services Understanding Kube-Proxy Service Handling Understanding Calico Native Service Handling Advertising Services ","date":"2022-08-26","objectID":"/certified-calico-operator/:2:0","tags":["calico","Project Calico","networking","Kubernetes","CloudNative","How-to"],"title":"Certified Calico Operator: Level1","uri":"/certified-calico-operator/"},{"categories":["How-to","technology"],"content":"Key highlights of the course: Self Paced Lab-Based Free Certification after competition of course Deep dive to K8s Networking and Security policy Calico CNI ","date":"2022-08-26","objectID":"/certified-calico-operator/:3:0","tags":["calico","Project Calico","networking","Kubernetes","CloudNative","How-to"],"title":"Certified Calico Operator: Level1","uri":"/certified-calico-operator/"},{"categories":["How-to","technology"],"content":"Let’s breakdown the course content: The course is self paced and nominally structured into 4 weeks. Each week will take around 2-4 hours to complete depending on your level of experience. So you can work at a relaxed pace one or two evenings a week, and still complete the whole course in 4 weeks. However, you are welcome to progress at whatever pace works best for you, including completing the whole course in a single day if you would like to. Each week covers a different area of learning and consists of a combination of short video presentations and step-by-step guided hands-on labs. There’s a short quiz at the end of each week so you can test your knowledge as you progress through the course. These do not count towards your certification and are there solely to help you cement your learning. Finally there is a graded exam at the end of the course that you must pass to obtain your certification. Week 1 Kubernetes Networking Introduction Lab Setup Installing Calico Installing the Sample Application Managing Your Lab Test Your Knowledge Week 2 Introduction to Network Policy Why use Calico for Network Policy Best Practices for Network Policy Network Policy Fundamentals Managing Trust Across Teams Network Policy for Hosts and NodePorts Week 3 Introduction Introduction to Pod Connectivity Pod Connectivity Fundamentals Encrypting Data in Transit IP Pools and BGP Peering Choosing the Best Networking Options Week 4 Introduction Introduction to Kubernetes Services Networking Understanding Kube-Proxy Understanding Calico Native Service Handling Advertising Services Test Your Knowledge ","date":"2022-08-26","objectID":"/certified-calico-operator/:4:0","tags":["calico","Project Calico","networking","Kubernetes","CloudNative","How-to"],"title":"Certified Calico Operator: Level1","uri":"/certified-calico-operator/"},{"categories":["How-to","technology"],"content":"Are there any prerequisites? The course assumes you are already reasonably familiar with the basic Kubernetes concepts such as pods and namespaces, though you definitely don’t have to be a Kubernetes expert. In-depth networking knowledge is not required. If you know what an IP address is, what DNS is, and what a load balancer is, then you know more than enough already. ","date":"2022-08-26","objectID":"/certified-calico-operator/:5:0","tags":["calico","Project Calico","networking","Kubernetes","CloudNative","How-to"],"title":"Certified Calico Operator: Level1","uri":"/certified-calico-operator/"},{"categories":["How-to","technology"],"content":"Why did I take this course? I come from the Networking background. Worked for Red Hat for about four years before moving on to Cloudera. During that time, I spent a significant amount of time on networking and began working with containers and kubernetes. I could therefore relate to Project calico’s products and solutions because I was familiar with networking, cloud native, and containers. This made me want to explore Calico’s cloud-native based open source solution on networking kubernetes. ","date":"2022-08-26","objectID":"/certified-calico-operator/:6:0","tags":["calico","Project Calico","networking","Kubernetes","CloudNative","How-to"],"title":"Certified Calico Operator: Level1","uri":"/certified-calico-operator/"},{"categories":["How-to","technology"],"content":"Taking the course: The course helped me revise some of the networking concepts such as kubernetes network policy, pod connectivity, Kubernetes DNS, outgoing NAT and some other key concepts such as IPv6 and dual stack, ingress and egress. Ofcourse, there are many more concepts throughout the course which explain in detail about Calico’s CNI, it’s working and advantages. ","date":"2022-08-26","objectID":"/certified-calico-operator/:7:0","tags":["calico","Project Calico","networking","Kubernetes","CloudNative","How-to"],"title":"Certified Calico Operator: Level1","uri":"/certified-calico-operator/"},{"categories":["How-to","technology"],"content":"Taking the exam: The exam has four sections: Kubernetes Networking, Networking Policy, Pod Connectivity, and Kubernetes Services. It has about 70 questions. If you understand the fundamentals of Kubernetes networking and have done your homework by studying the course well, you should be able to pass the exam on your first try. There are a variety of question types, including true or false, multiple choice, and while some questions have multiple options with two or more than two correct answers. The exam tests your overall understanding about the concepts covered in the course content. You can check your score after submitting all of your responses, and if you passed, you can download the certificate. ","date":"2022-08-26","objectID":"/certified-calico-operator/:8:0","tags":["calico","Project Calico","networking","Kubernetes","CloudNative","How-to"],"title":"Certified Calico Operator: Level1","uri":"/certified-calico-operator/"},{"categories":["How-to","technology"],"content":"My key takeaways from the course: Calico supports both namespaced network policy, and a non-namespaced global network policy. Unlike Kubernetes, where the action is implicit allow, Calico allows to explicitly specify allow or deny actions. Calico can be installed as part of the hosted kubernetes platform on all major cloud providers such as EKS, AKS and GKE. Calico can provide both overlay and non-overlay networking. I would like to strongly recommend the Certified Calico Operator: Level 1 course for everyone interested in Kubernetes networking. ","date":"2022-08-26","objectID":"/certified-calico-operator/:9:0","tags":["calico","Project Calico","networking","Kubernetes","CloudNative","How-to"],"title":"Certified Calico Operator: Level1","uri":"/certified-calico-operator/"},{"categories":["How-to","technology"],"content":"Explore other certifications from Project Calico: Certified Calico Operator: eBPF Certified Calico Operator: AWS Expert Certified Calico Operator: Azure Expert Next up, I will be studying the Certified Calico Operator: AWS Expert course and taking the exam. Hope this article was helpful. Stay tuned for the next blog and happy learning!!👋🏻 ","date":"2022-08-26","objectID":"/certified-calico-operator/:10:0","tags":["calico","Project Calico","networking","Kubernetes","CloudNative","How-to"],"title":"Certified Calico Operator: Level1","uri":"/certified-calico-operator/"},{"categories":["community","UnconventionalContributors"],"content":"We have many diverse contributors in opensource that help upstream communities in unconventional ways, which doesn’t require any coding or development skills. Starting with a non-code contribution can help anyone overcome the sense of failure and not being good enough, and it can also serve as a springboard for our open source adventure. This interview series aims to highlight some non-code open source contributions that anyone can make right now to get started contributing. For this month’s edition, we talked about open source principles of contribution and collaboration with Harish Pillay. Thank you for taking some time out and interacting with us, Harish. You have been at Red Hat for as long as I can remember. Can you give us some background about what your journey has been at Red Hat and what you do today? I’m Harish Pillay and I’m a Principal Community Architect based out of the Red Hat Singapore office. I’ve been at Red Hat now for almost 19 years. Yes, it is a long journey, but meaningful from my point of view. The bigger picture is what needs to be kept in mind as we go about enabling and empowering open source technologies and software. This is coupled with technical collaboration both in software and hardware. The Red Hat Singapore office was setup in my SOHO (small office home office) in 2003. In 2004, the Singapore office became the Asia-Pacific headquarters with the help of the Singapore Economic Development Board. I had, at the start, been running Red Hat Singapore as a one-man show as the Chief Technology Architect. I did everything from tech support to sales, to marketing, to speaking at conferences, etc. It was a little simpler because we only had to deal with one product - Red Hat Enterprise Linux. And since then, as we know, Red Hat has evolved and open source solutions got deeper into the proprietary software stack. You find a lot more services and options showing up as open source now, whether from us or someone else. So, from my point of view, it was not so much about a contribution to open source projects in and of itself directly. It is about building the bigger ecosystem of opportunities. It’s almost a lifetime that you have dedicated to the Red Hat Singapore Office and building an open source ecosystem. What has your experience been in being a contributor? My earliest code contribution was sometime in 1982 – see the blog post about stuff done then: https://harishpillay.wordpress.com/2018/06/03/and-they-are-online-now/ I have contributed to so many places, I don’t even know where it was, so I actually have no easy way to track it. These were the days before you had things like GitHub and GitLab and things like that. From a contributor’s standpoint, my claim to fame dates back to 1999. What happened in 1999 specifically just before Red Hat IPO’ed on Nasdaq, the Red Hat leadership and marketing team then looked at all the code that was being shipped at that time, which was Red Hat Linux, and extracted the email addresses and contributor names - from how-tos, kernel code, in Apache, etc and whatever was inside the distribution. These were collated into a contributors list and emails were sent to each one offering shares in Red Hat stock! Red Hat was the first one to do such a thing then (it was repeated by VA Linux some months later when VA Linux IPO’ed). So I received an email saying that I have ten shares from this entity called “Red Hat”. Who was this company? Frankly, I had no idea. And why did I receive it? All because I had written code and I forgot about it. I had written a how-to on IP aliases. It was a very pleasant and thoughtful gesture of Red Hat to acknowledge contributors. Those docs/how-tos have been translated into many languages, and I’ve received email requests for help in French, Mandarin, Spanish, Japanese, and Russian. IP aliases was about having one network card to respond to multiple IP addresses. This was a tricky thing to do, but IP aliases allowed you to then have one server to s","date":"2022-08-22","objectID":"/contributorblog_3/:0:0","tags":["blogs","community","Free Software","Open Source"],"title":"Contributor Blog #3: Interview with Harish Pillay","uri":"/contributorblog_3/"},{"categories":["How-to","technology"],"content":"I started exploring and using Jupyter Notebook earlier this year for a project based on opensource enablement, licensing and operate first. This is the first time ever I was working with a notebook and apparently had to start from sratch right from understanding how it works, how it needs to be installed and finally how it is to be pushed to a repo and publish the work. In the process of exploring the tool, I went throught quite a handful of articles and videos. Everyone’s way was a bit different than the other and there was no way correct or proper to work around this. Amongst all, I found the below way to be the most effective and clean and hence thought of penning down the learnings create a how-to guide on installing and working with Jupyter Notebooks. This article is written based on a MacOS. Haven’t tried on a linux machine. Jupyter Notebook = JN //for ease of writing Prerequisites: Install python3 package ","date":"2022-07-21","objectID":"/jupyternotebooks/:0:0","tags":["JupyterNotebooks","How-to"],"title":"Getting started with Jupyter Notebooks","uri":"/jupyternotebooks/"},{"categories":["How-to","technology"],"content":"Install and launch JN First, let’s create a virtual environment to install JN parth@mac Desktop % python3 -m venv jupyter parth@mac Desktop % ls jupyter This creates a folder called jupyter in the current dir which has a python virtual env. To turn the venv on: parth@mac Desktop % source ~/jupyter/bin/activate (jupyter) parth@mac Desktop % The jupyter in parenthesis lets us know that we are in our python jupyter virtual environment. NOTE: This needs to be turned everytime you need to work with jupyter notebooks. Now, let’s install the jupyter notebook pkg (jupyter) parth@mac Desktop % pip3 install jupyter notebook This installs the jupyter and notebook packages. Creating another dir where I will be launching the JN from and where I can create pages. (jupyter) parth@mac Desktop % mkdir JupyterNotebooks (jupyter) parth@mac Desktop % cd JupyterNotebooks (jupyter) parth@mac JupyterNotebooks % pwd /Users/parth/Desktop/JupyterNotebooks Launching JN now (jupyter) parth@mac JupyterNotebooks % jupyter notebook . This runs a kernel process on the terminal and launches JN from the JupyterNotebooks dir in your default web browser. Note All and any work you do can be accessed via localhost and is saved locally. Click on the new button in the brower’s JN page to create a new page. The page can be a simple text file, a python file with .ipynb extension or markdown page with .md extention. Any pages you create are now saved in JupyterNotebooks dir. Alright, we have installed and launched JN to create the pages. Now it’s time we create a bundled book out of the pages and upload it in a git repo and publish the book. ","date":"2022-07-21","objectID":"/jupyternotebooks/:1:0","tags":["JupyterNotebooks","How-to"],"title":"Getting started with Jupyter Notebooks","uri":"/jupyternotebooks/"},{"categories":["How-to","technology"],"content":"Build and Publish the book Follow the steps mentioned here https://jupyterbook.org/en/stable/start/overview.html Install Jupyter Book pip3 install -U jupyter-book Tip Jupyter Book comes bundled with a lightweight sample book to help you understand a book’s structure. Create a sample book by running the following command: $ jupyter-book create mynewbook/ The name mynewbook can be replaced with any other name and create option will create a skeleton of book for you. Edit/add/modify the pages under this book. Once you are done with creating pages and have configured _toc.yml and _config.yml, it’s time to build the book. $ jupyter-book build mynewbook/ Use the same build option to re-build the book if you make any new changes. The book is build. Once all the final reviews are done, it can be published in the git repository. Follow the link below to publish the book online. https://jupyterbook.org/en/stable/start/publish.html Checkout some of my works based on Jupyter Notebook Opensource Enablement Opensource Licensing Thank you for reading! ","date":"2022-07-21","objectID":"/jupyternotebooks/:2:0","tags":["JupyterNotebooks","How-to"],"title":"Getting started with Jupyter Notebooks","uri":"/jupyternotebooks/"},{"categories":["community","UnconventionalContributors"],"content":"We have many diverse contributors in opensource that help upstream communities in unconventional ways, which doesn’t require any coding or development skills. Starting with a non-code contribution can help anyone overcome the sense of failure and not being good enough, and it can also serve as a springboard for our open source adventure. This interview series aims to highlight some non-code open source contributions that anyone can make right now to get started contributing. For this month’s edition, we talked about Mozilla and how a new contributor can get started with their first contributions, with Priyanka Pandey. Can you tell our readers a little bit about yourself, your experience at Red Hat, and what you do now? I was introduced to the world of Open Source back in my college days. My first Open Source contribution was translating Firefox strings from English to my native language, Bengali. During the span of my college life, I had the opportunity to volunteer with a few different communities and meet some very interesting and inspiring people from the Open Source world. It was during a few of these interactions that I happened to meet some Red Hatters, who were extremely passionate about Open Source. Red Hat, a for-profit organization, was generating revenue year-on-year and still contained the soul of Open Source at its heart; it was rare and impressive. Red Hat became my dream from that moment forward!The tiny adult that I was back then, started aspiring to be a Red Hatter. The dream turned into reality soon when I noticed an opening for a Technical Writer position at the Red Hat Pune office, back in 2015. I joined Red Hat as an Associate Technical Writer. Before joining Red Hat as an employee, I had visited the Red Hat office a couple of times for different community events. Whenever a visitor accesses any of the Red Hat office premises, they are given a temporary visitor pass. My friends still tease me, saying that I was using so many of those temporary passes that the organization got tired and decided to give me a permanent employee ID card. Well, I am definitely not complaining! :P Achieving the dream was only the start of a beautiful journey, a journey that is still ongoing. For the initial four years at Red Hat, I did write technical documents for the middleware product line of Red Hat, mainly EAP. As a writer, we were seated along with the technical support engineers of the product and would be collaborating with them very closely to understand customer requirements. During these collaborations, I realized that the opportunity to directly interact with customers everyday, to be able to help resolve their technical issues, had a different level of work satisfaction. When I shared my thoughts with my writing team’s manager, I was surprised to see the level of support and mentoring that he provided which helped me land an internal opening within the Technical Support team of Red Hat. In 2019, I took up the role of a Technical Lead (aka, Support Operations Lead) within the RHEL support team. From there began another phase of the journey. Being on our toes for all days, 24x7; handling customer escalation; firefighting difficult customer crisis situations, the more challenges that came in, the more my enthusiasm and energy grew. In July of 2020, I became an Associate Manager within the RHEL support team and exactly 2 years later and around 15 days back, I got promoted to be a Manager for the same team now. Well, congratulations on your promotion in that case. While visiting the Red Hat office for community events, I am sure you wouldn’t have ever thought of becoming a manager at Red Hat. On that note, could you tell me about the product(s) you’ve contributed to so far? When it comes to open source contributions, I was primarily involved with the Mozilla community. From translating Firefox strings to fixing simpler coding bugs; from evangelizing Mozilla products at different forums to writing documents for the Mozill","date":"2022-07-20","objectID":"/contributorblog_2/:0:0","tags":["blogs","community","mozilla","wikimedia","outreachy"],"title":"Contributor Blog #2: Interview with Priyanka Pandey","uri":"/contributorblog_2/"},{"categories":["VisitingAuthors","How-to","technology"],"content":" Ansible is an open-source configuration management and deployment tool. Below are the steps to run docker container on AWS EC2 instances using Ansible playbook. Create AWS EC2 instances (master and slave) Update the security group of slave exposing port 8080 Install Ansible on master Install python on slave Update /etc/ansible/hosts on master with the slave IP Create a dockerfile which will be used to create customized image Create an ansible playbook with detailed tasks Execute the ansible playbook Check if the docker container is running successfully on the slave machine ","date":"2022-07-09","objectID":"/docker_containters_ansibleaws/:0:0","tags":["How-to","docker","blog","ansible","aws","technology","VrindaHegde","SiyaAmonkar","VisitingAuthors"],"title":"Docker containers using Ansible on AWS","uri":"/docker_containters_ansibleaws/"},{"categories":["VisitingAuthors","How-to","technology"],"content":"Install Ansible on master machine: Execute the below commands to install Ansible on the master machine: sudo apt-get update sudo apt install software-properties-common sudo apt-add-repository ppa:ansible/ansible sudo apt-get install ansible ","date":"2022-07-09","objectID":"/docker_containters_ansibleaws/:1:0","tags":["How-to","docker","blog","ansible","aws","technology","VrindaHegde","SiyaAmonkar","VisitingAuthors"],"title":"Docker containers using Ansible on AWS","uri":"/docker_containters_ansibleaws/"},{"categories":["VisitingAuthors","How-to","technology"],"content":"Install Python on slave machine: Use the below commands to install python on the slave machine: sudo apt-get update sudo apt-get install python On the master machine perform the below steps to create ssh-keygen: cd /home/ubuntu/.ssh ssh-keygen File names id_rsa.pub will be generated after executing ssh-keygen on master machine. on master machine Copy the contents of the id_rsa.pub file and paste it on slave machine inside the authorized_keys file located at /home/ubuntu/.ssh on slave machine ","date":"2022-07-09","objectID":"/docker_containters_ansibleaws/:2:0","tags":["How-to","docker","blog","ansible","aws","technology","VrindaHegde","SiyaAmonkar","VisitingAuthors"],"title":"Docker containers using Ansible on AWS","uri":"/docker_containters_ansibleaws/"},{"categories":["VisitingAuthors","How-to","technology"],"content":"Update the hosts file on master: To establish ssh connection with the slave machine, add the IP of the slave machine on master’s hosts file. To edit the hosts file use the below command: sudo nano /etc/ansible/hosts Paste the below content at the end of the file: [production] slave ansible_ssh_host=\u003cslave-IP\u003e To verify the ssh connection between the master and slave, execute the below command: output of the ping command on master machine ","date":"2022-07-09","objectID":"/docker_containters_ansibleaws/:3:0","tags":["How-to","docker","blog","ansible","aws","technology","VrindaHegde","SiyaAmonkar","VisitingAuthors"],"title":"Docker containers using Ansible on AWS","uri":"/docker_containters_ansibleaws/"},{"categories":["VisitingAuthors","How-to","technology"],"content":"Creating the code directory on master machine: Create a directory named LAMP_STACK_content at /home/ubuntu/ location on the master machine. Create a dockerfile at this location. ","date":"2022-07-09","objectID":"/docker_containters_ansibleaws/:4:0","tags":["How-to","docker","blog","ansible","aws","technology","VrindaHegde","SiyaAmonkar","VisitingAuthors"],"title":"Docker containers using Ansible on AWS","uri":"/docker_containters_ansibleaws/"},{"categories":["VisitingAuthors","How-to","technology"],"content":"Create a dockerfile: In order to create a customized image we will create a dockerfile with the following contents: # Dockerfile for LAMP Stack installation # Ubuntu 18.04 image FROM ubuntu:18.04 ENV DEBIAN_FRONTEND=noninteractive RUN apt-get update -y RUN apt-get upgrade -y # Install apache RUN apt-get install -y apache2 # Prerequisites for installing php7.3 RUN apt-get install -y software-properties-common RUN add-apt-repository ppa:ondrej/php RUN apt install -y php7.3-fpm # Install php7.3 for this set up RUN apt install -y php7.3 # Extensions of php RUN apt install php7.3-common php7.3-mysql php7.3-xml php7.3-xmlrpc php7.3-curl php7.3-gd php7.3-imagick php7.3-cli php7.3-dev php7.3-imap php7.3-mbstring php7.3-opcache php7.3-soap php7.3-zip php7.3-intl -y # Removing the default index.html page and copying the project code RUN rm -f /var/www/html/index.html COPY . /var/www/html/ # Install ufw RUN apt install ufw -y RUN ufw app list # install library RUN apt-get install libapache2-mod-php7.3 # install additional packages RUN a2dismod mpm_event \u0026\u0026 a2enmod mpm_prefork \u0026\u0026 a2enmod php7.3 # Restart apache RUN service apache2 restart # Provide executable permissions to the code RUN chmod -R 0777 /var/www/html/* RUN chmod -R 0777 /var/* # Change WORKDIR WORKDIR /var/www/html CMD [\"apachectl\",\"-D\",\"FOREGROUND\"] RUN a2enmod rewrite EXPOSE 80 EXPOSE 443 ","date":"2022-07-09","objectID":"/docker_containters_ansibleaws/:5:0","tags":["How-to","docker","blog","ansible","aws","technology","VrindaHegde","SiyaAmonkar","VisitingAuthors"],"title":"Docker containers using Ansible on AWS","uri":"/docker_containters_ansibleaws/"},{"categories":["VisitingAuthors","How-to","technology"],"content":"Create ansible playbook: Create an ansible-playbook-lamp-stack-new.yaml file on master with the below content. --- - hosts: slave become: yes gather_facts: no tasks: - name: create build directory file: path: /root/demo-lamp_stack state: directory owner: root group: root mode: '0755' - name: copy Dockerfile copy: src: /home/ubuntu/LAMP_STACK_content dest: /root/demo-lamp_stack/ owner: root group: root mode: '0644' - name: build and push container image community.docker.docker_image: build: path: /root/demo-lamp_stack/LAMP_STACK_content/ name: dock1998/lamp_stack_new:v1 source: build state: present - name: Running the container community.docker.docker_container: image: dock1998/lamp_stack_new:v1 name: lamp_stack_new_cont state: started ports: \"8080:80\" - name: Tag and push to docker hub community.docker.docker_image: name: dock1998/lamp_stack_new:v1 repository: dock1998/lamp_stack_new:v2 push: yes source: local state: present ","date":"2022-07-09","objectID":"/docker_containters_ansibleaws/:6:0","tags":["How-to","docker","blog","ansible","aws","technology","VrindaHegde","SiyaAmonkar","VisitingAuthors"],"title":"Docker containers using Ansible on AWS","uri":"/docker_containters_ansibleaws/"},{"categories":["VisitingAuthors","How-to","technology"],"content":"Execute the ansible playbook: Note: Before proceeding ahead with the next steps, you will need to perform docker login using the dockerhub account on the slave machine from where the image will be pushed and pulled. To execute the playbook use the below command: ansible-playbook ansible-playbook-lmap-stack-new.yaml output of the above command on master machine ","date":"2022-07-09","objectID":"/docker_containters_ansibleaws/:7:0","tags":["How-to","docker","blog","ansible","aws","technology","VrindaHegde","SiyaAmonkar","VisitingAuthors"],"title":"Docker containers using Ansible on AWS","uri":"/docker_containters_ansibleaws/"},{"categories":["VisitingAuthors","How-to","technology"],"content":"To verify the execution of playbook: Check if the code directory is copied on the slave machine. output on slave Check if the docker container is running on the slave machine: docker ps output of docker ps on the slave machine Note: If you get permission denied error after executing docker ps command then you need to give permissions to the docker.sock file by executing the below command: chmod -R 0777 /var/run/docker.sock On the slave machine add the in-bound rules to direct traffic to the port 8080 by updating the security groups on the AWS console as shown below. AWS console for slave machine Access the slave IP with the exposed port 8080 on the browser and the application should be up. Application running on port 8080 Hope the article was useful, thanks for reading! Happy learning. 👍 Vrinda Hegde is a DevOps Engineer, who likes to explore orchestration tools and automate the process of deploying containerized applications. She likes to share her findings by writing articles on medium.com. She can be reached out on LinkedIn or via email Siya Amonkar is a DevOps Engineer who likes to explore new tools and technologies. She can be reached out on LinkedIn or via email ","date":"2022-07-09","objectID":"/docker_containters_ansibleaws/:8:0","tags":["How-to","docker","blog","ansible","aws","technology","VrindaHegde","SiyaAmonkar","VisitingAuthors"],"title":"Docker containers using Ansible on AWS","uri":"/docker_containters_ansibleaws/"},{"categories":null,"content":"Hey there! TL;DR: Name: Parth Goswami Let’s connect on Twitter ","date":"2022-06-26","objectID":"/about/:0:0","tags":null,"title":"About","uri":"/about/"},{"categories":null,"content":"About the website A platform for the community, by the community VistingAuthors Many of my colleagues are avid writers who frequently document their knowledge and experiences. Some of them use their own platforms to publish the information, while others use blogs like Medium and Blogger to publish it. The VisitingAuthors area of this website is an effort to offer one such platform for authors to share their knowledge and ideas. Please check out the site; I hope it will provide you with useful content. UnconventionalContributors We have a diverse range of contributors in the open source community who make contributions in unconventional ways without using any coding or developing expertise, some ways we hear quite often, some not so much. This high-level instructional interview series should be beneficial to anyone who wants to start contributing to open source. One such forum is the UnconventionalContributors series, which is designed for aspiring and emerging contributors to get involved with open source communities. ","date":"2022-06-26","objectID":"/about/:1:0","tags":null,"title":"About","uri":"/about/"},{"categories":null,"content":"About the author I am a free and open source software enthusiast and a community ambassador of Project Calico. Currently, I am a Customer Enablement Engineer working for Cloudera on the Professional Services Team in Bengaluru, India. My interests are in Kubernetes, cloud, and cloud native related projects. I like to contribute to OpenSource in any possible way. I reguarly host meetups and speak in conferences about the concepts that I’ve learned or am currently learning. Here’s the list of talks delivered in various conferences and meetups I have open sourced the platform’s code in this repo for folks who are interested in creating their own. The opinions stated here are my own, not necessarily those of my company. ","date":"2022-06-26","objectID":"/about/:2:0","tags":null,"title":"About","uri":"/about/"},{"categories":null,"content":"I’m available for Mentoring | Beta testing new products | Speaking at Events | Technical Writing | Open Source Contributions | Guest lecturing ","date":"2022-06-26","objectID":"/about/:3:0","tags":null,"title":"About","uri":"/about/"},{"categories":null,"content":"About copyright All original articles on this site are protected by the Creative Commons Attribution-NonCommercial 4.0 License/CC BY-NC 4.0 . Copyright statement You are free to: Share — copy and redistribute the material in any medium or format Adapt — remix, transform, and build upon the material The licensor cannot revoke these freedoms as long as you follow the license terms. Any individual or media should abide by the following copyright requirements when reproducing the original content of this site (including text, self-made images, and photographic works): indicate reprint indicate the source as the site domain name ( parthgoswami.com ), or the full URL where the reprinted content is located NonCommercial — You may not use the material for commercial purposes. Except for original works, most of the pictures on this site come from the Internet. The original copyright owner of such pictures may request this site to stop using relevant pictures at any time and for any reason, including pictures edited by this site (such as annotated descriptions). ","date":"2022-06-26","objectID":"/about/:4:0","tags":null,"title":"About","uri":"/about/"},{"categories":null,"content":"Come, say hello!👋🏻 Get in touch with us, we would love to talk to you. ","date":"2022-06-26","objectID":"/about/:5:0","tags":null,"title":"About","uri":"/about/"},{"categories":["community","UnconventionalContributors"],"content":"We have many diverse contributors in opensource that help upstream communities in unconventional ways, which doesn’t require any coding or development skills. Starting with a non-code contribution can help anyone overcome the sense of failure and not being good enough, and it can also serve as a springboard for our open source adventure. This interview series aims to highlight some non-code open source contributions that anyone can make right now to get started contributing. For this month’s edition, we talked about the Mozilla and Wikipedia with Prathamesh Chavan. Can you tell us some background about you, what your journey has been at Red Hat, and what you do today? I am Prathamesh Chavan and I completed my engineering degree in information technology in 2016 and joined Red Hat as an intern on October 3rd, 2016. I was converted to full-time employment in 2017 and joined the Technical Chat Support team as an Associate Customer Support Specialist-Technical. As a Technical Support Engineer, I worked on queries related to subscription management, and it was in March of 2020 when I joined the then CEE Operations team as a Technical Project Coordinator. As of today, I am an Associate Technical Project Manager in the CEE Strategic Solutions team, and my day- to-day responsibilities involve managing agile projects. Share your experience in becoming a community contributor. An open source contribution is a selfless effort to do something good. After becoming an open source contributor, I was able to explore the various steps of the software development life cycle. I learned how it provides a basis for project planning, scheduling, and estimating, as well as how it raises project planning visibility among all the stakeholders. I felt very empowered and motivated due to the open source ideology, and this has helped me in building a “never stop, never-give-up” attitude. What project(s) have you contributed to? How did your contribution journey start? I have contributed to Mozilla projects and Wikipedia. I joined the technical club at my university as an events blogger, and it was only after that that I came to know about the various open source communities and the contribution pathways. In my initial days, I read a couple of articles in the Mozilla Developers Network portal related to the Mozilla Firefox browser and started to suggest edits wherever needed. Later, I started localizing the Mozilla articles, which were written in English, into my native language, Marathi. Over the next couple of months, I got acquainted with the Mozilla Firefox browser and started helping other users fix their issues with the browser. I also believe most open source organizations consider answering other people’s questions on Quora or Reddit to be a useful contribution. I started joining and reading discussions on threads and learned a lot through that. So, I always recommend to people that if we notice a question and know the solution, we may try to help the person who asked it by answering it, and our responses will be counted as contributions to the project. Sometimes, assuming one doesn’t know the perfect solution to the problem, simply helping others comprehend why the problem arises may be enough to allow them to come up with their own solution. You may help manage the discussion threads or community chat channels by answering questions about problems on GitHub, opensource.com, Mozilla, etc. In what way(s) have you contributed? As someone who likes talking about open source philosophies and concepts, I decided to start with a documentation-related contribution as a technical writer, where I started writing and reviewing the documentation on the Mozilla Developers Network. Later, I realized that the articles were only written in English, and there was a huge need to localize such articles in the regional languages. I started my contribution by localizing the articles and strings. Due to such contributions, I got some knowledge about how to use the Fi","date":"2022-06-15","objectID":"/contributorblog_1/:0:0","tags":["blogs","community","fedora","mozilla"],"title":"Contributor Blog #1: Interview with Prathamesh Chavan","uri":"/contributorblog_1/"},{"categories":["How-to","VisitingAuthors","technology"],"content":"Recently, while working on an internal project I was required to establish network connection between a Windows server and a RHEL server. Sounds easy, right? It would have been indeed, however, the task was not only to establish the network but also to ping the Windows server from RHEL via ansible, and that’s where the scenario got tricky. Also, the servers were EC2 instances hosted on AWS. I went through a few articles and videos on and thought of documenting my learnings in this blog. Amazon Elastic Compute Cloud (Amazon EC2) provides scalable computing capacity in the Amazon Web Services (AWS) Cloud. Using Amazon EC2 eliminates your need to invest in hardware up front, so you can develop and deploy applications faster. You can use Amazon EC2 to launch as many or as few virtual servers as you need, configure security and networking, and manage storage. Amazon EC2 enables you to scale up or down to handle changes in requirements or spikes in popularity, reducing your need to forecast traffic. Ping is a command-line utility, available on virtually any operating system with network connectivity, that acts as a test to see if a networked device is reachable. The ping command sends a request over the network to a specific device. A successful ping results in a response from the computer that was pinged back to the originating computer. Let’s start by getting to know the host requirements. Host requirements: For Ansible to communicate to a Windows host and use Windows modules, the Windows host must meet these requirements: Ansible can generally manage Windows versions under current and extended support from Microsoft. Ansible can manage desktop OSs including Windows 7, 8.1, and 10, and server OSs including Windows Server 2008, 2008 R2, 2012, 2012 R2, 2016, and 2019. Ansible requires PowerShell 3.0 or newer and at least .NET 4.0 to be installed on the Windows host. A WinRM listener should be created and activated. Use below steps to configure: ","date":"2022-06-11","objectID":"/ansible_sd/:0:0","tags":["How-to","technology","aws","windows","Red Hat","RHEL","ansible","ShreyaDhange","VisitingAuthors"],"title":"Managing Windows Server from Red Hat Server using Ansible on AWS EC2","uri":"/ansible_sd/"},{"categories":["How-to","VisitingAuthors","technology"],"content":"1. Configure Windows server 2016: a. Open Windows PowerShell and check the version: PS C:\\Users\\Adminstrator\u003e Get-Host | Select-Object version powershell version The powershell version should at least be 3.0 or more. If not then upgrade it using this document. Since we have version 5.1 no need to upgrade the version. b. Once PowerShell has been upgraded, the final step is for the WinRM service to be configured so that Ansible can connect to it: PS C:\\Users\\Administrator\u003e [Net.ServicePointManager]::SecurityProtocol = [Net.SecurityProtocolType]::Tls12 PS C:\\Users\\Administrator\u003e $url = \"https://raw.githubusercontent.com/ansible/ansible/devel/examples/scripts/ConfigureRemotingForAnsible.ps1\"$file = \"$env:temp\\ConfigureRemotingForAnsible.ps1\" PS C:\\Users\\Administrator\u003e (New-Object -TypeName System.Net.WebClient).DownloadFile($url, $file) PS C:\\Users\\Administrator\u003e powershell.exe -ExecutionPolicy ByPass -File $file c. Run this below script on Windows PowerShell ISE and check the version after successful script completion: Param([string]$computerName) Function enableWinRM { $result = winrm id -r:$global:compName 2\u003e$null Write-Host if ($LastExitCode -eq 0) { Write-Host \"WinRM already enabled on\" $global:compName \"...\" -ForegroundColor green } else { Write-Host \"Enabling WinRM on\" $global:compName \"...\" -ForegroundColor red .\\pstools\\psexec.exe \\\\$global:compName -s C:\\Windows\\system32\\winrm.cmd qc -quiet if ($LastExitCode -eq 0) { .\\pstools\\psservice.exe \\\\$global:compName restart WinRM $result = winrm id -r:$global:compName 2\u003e$null if ($LastExitCode -eq 0) {Write-Host 'WinRM successfully enabled!' -ForegroundColor green} else {exit 1} } else {exit 1} } } $global:compName = $computerName enableWinRM exit 0 version after script completion d. Check if ports are listening: PS C:\\Users\\Administrator\u003e winrm enumerate winrm/config/Listener port listener ","date":"2022-06-11","objectID":"/ansible_sd/:1:0","tags":["How-to","technology","aws","windows","Red Hat","RHEL","ansible","ShreyaDhange","VisitingAuthors"],"title":"Managing Windows Server from Red Hat Server using Ansible on AWS EC2","uri":"/ansible_sd/"},{"categories":["How-to","VisitingAuthors","technology"],"content":"2. Configure Red Hat 8 Server: a. If you have subscription manager account then subscribe your system using subscription-manager command: [root@ip-172-31-23-177 ~]# subscription-manager register b. Install ansible if package is not available: [root@ip-172-31-23-177 ~]# yum install ansible c. Install python-pip package: [root@ip-172-31-23-177 ~]# pip2 --version [root@ip-172-31-23-177 ~]# pip3 --version [root@ip-172-31-23-177 ~]# dnf install python2-pip [root@ip-172-31-23-177 ~]# dnf install python3-pip [root@ip-172-31-23-177 ~]# pip2 --version pip 9.0.3 from /usr/lib/python2.7/site-packages (python 2.7) [root@ip-172-31-23-177 ~]# pip3 --version pip 9.0.3 from /usr/lib/python3.6/site-packages (python 3.6) [root@ip-172-31-23-177 ~]# pip3 install \"pywinrm\u003e=0.2.2\" d. Now write a ansible playbook to ping windows server: [root@ip-172-31-23-177 ~]# tail /etc/ansible/hosts ## db-[99:101]-node.example.com [windows] 107.20.75.188 [windows:vars] ansible_user=\"windows_username\" //for example: ansible_user=\"Administrator\" ansible_password=\"windows_user_password\" ansible_connection=winrm ansible_winrm_server_cert_validation=ignore e. Use the below command to ping windows server: [root@ip-172-31-23-177 ansible]# ansible all -i hosts -m win_ping pinging windows server ","date":"2022-06-11","objectID":"/ansible_sd/:2:0","tags":["How-to","technology","aws","windows","Red Hat","RHEL","ansible","ShreyaDhange","VisitingAuthors"],"title":"Managing Windows Server from Red Hat Server using Ansible on AWS EC2","uri":"/ansible_sd/"},{"categories":["How-to","VisitingAuthors","technology"],"content":"Resources: Setting up a Windows Host WinRM Setup WinRM setup script Linux and Windows host setup How to install pip in RHEL 8 / CentOS 8 step by step instructions Windows PowerShell Upgrade Ansible-windows-lab-setup (where ansible server is in linux and target node is in windows) Tip: RDP Port No: 3389 Shreya Dhange is a Technical Training Developer at Red Hat, who likes to explore and learn new technologies and share her knowledge by writing articles. She has completed her Masters in Computer Science and has gained award for her exemplary academic performance. She has been engaged in creating and delivering content in the cloud and linux space. She can be reached out LinkedIn or via email. ","date":"2022-06-11","objectID":"/ansible_sd/:3:0","tags":["How-to","technology","aws","windows","Red Hat","RHEL","ansible","ShreyaDhange","VisitingAuthors"],"title":"Managing Windows Server from Red Hat Server using Ansible on AWS EC2","uri":"/ansible_sd/"},{"categories":["community","VisitingAuthors","CloudNative"],"content":"KubeCon + CloudNativeCon in Barcelona, Spain (from 20 to 23rd May 2019) was my first international conference invite. KubeCon is kind of a conference where you will meet all cloud-native enthusiasts, researchers, engineers and everyone working to build cloud-native applications easier. I applied for a Diversity Scholarship and fortunately, I was amongst the 56 people selected to be a part of KubeCon 2019 at Barcelona. In KubeCon, you get to be a part of Keynotes, breakout sessions and get to attend co-located workshops, panel discussions and many more events. Everything runs in parallel so you need to manage and prioritize your schedule. Registration for most of the co-located events starts a month prior to the conference date. There were multiple sessions to pick from, mostly centered around how people use cloud native opensource tools to improve certain deliverables, mistakes they might have made using the cloud native tools and how we shouldn’t do what they did or just an introduction to new ways of doing things. There were sessions on Cloud Native projects, Lightning talks on diverse topics and also an opportunity to meet maintainers of some of the tools we use in the cloud native space. On Day1, I have attended the “Kubernetes Operator Framework Workshop” which was designed and delivered by Red Hat Engineers. This workshop helped me in building the Operator fundamentals and post workshop I got an opportunity to discuss real time issues and scenarios. Resources used for this workshop can be found at: https://learn.openshift.com [Building Operators on OpenShift] From Day2, I attended breakout sessions and booth demos. These are some of the sessions I’ve attended and found interesting: Modern CI/CD with Tekton and Prow Automated via Jenkins X Kubeflow: Bringing CloudNative Platform for ML to Kubernetes Scale Kubernetes Service Endpoints 100x Real World Kubernetes Deployment Patterns Kubernetes Metrics API using Prometheus Furthermore, I did participate in Diversity lunch and learn activity sponsored by Google. The agenda was to meet people with common interest in the provided components, topics and sit in round table fashion, discuss things and find out ongoing challenges, best practices, and next actions based on each team discussion. I was a part of “Monitoring Team” where we discussed about monitoring and tracing in cloud infrastructure. At the end, we concluded with Prometheus as the most promising tool. KubeCon focus and role of CNCF: The major role and focus of CNCF for such events is towards Open Source Contributions, adapting community driven projects, graduating them and make it usable to solve real world problems. Once it is available in the community, people would start using it in small scale environment to the enterprise level and share their best practices, failures, research in such conferences. Some of the great Projects which were adopted and graduated by CNCF are: Kubernetes Prometheus Fluentd Etcd This list keep on increasing as it gets more support from the community: https://www.cncf.io/projects/ Overall, KubeCon was a dream experience for me. I gained a lot of knowledge and met many people who are always ready to share and help! When community provide you such life-changing opportunity, then contributing to the community becomes more of a passion than a job. To locate all the adventures, you can view the awesome photos from the event here. The video playlist for the sessions including keynotes, lightning talks can be found here Rutvik Kshirsagar is working as a Sr.Tech Support Engineer at Red Hat. He is responsible for providing solutions to the clients who are using hybrid cloud ecosystems at scale. Primarily he works on OpenShift Container Platform and Stackrox for delivering devsecops best practices that satisfies the mission critical workload requirements. Apart from the day to day work, he enjoys reading technical blogs, coding, and following opensource activities that are happening across the glob","date":"2022-06-11","objectID":"/kubecon_exp/:0:0","tags":["CNCF","KubeCon","blog","community","VisitingAuthors","RutvikKshirsagar"],"title":"My experience with CNCF KubeCon","uri":"/kubecon_exp/"},{"categories":["VisitingAuthors","How-to","technology"],"content":"Let us create a dockerfile for deploying the application. In this dockerfile we will use ubuntu image as the base image and then install apache and php above it. Then we will create a MYSQL container which will be connected to our application. ","date":"2022-06-06","objectID":"/dockerizing_lamp_stack_app/:0:0","tags":["How-to","docker","blog","lamp stack","technology","VrindaHegde","VisitingAuthors"],"title":"Dockerizing LAMP Stack Application","uri":"/dockerizing_lamp_stack_app/"},{"categories":["VisitingAuthors","How-to","technology"],"content":"Below is the Dockerfile snippet: # Dockerfile for LAMP Stack installation # Ubuntu 18.04 image FROM ubuntu:18.04 ENV DEBIAN_FRONTEND=noninteractive RUN apt-get update -y RUN apt-get upgrade -y # Install apache RUN apt-get install -y apache2 # Prerequisites for installing php7.3 RUN apt-get install -y software-properties-common RUN add-apt-repository ppa:ondrej/php RUN apt install -y php7.3-fpm # Install php7.3 for this set up RUN apt install -y php7.3 # Extensions of php RUN apt install php7.3-common php7.3-mysql php7.3-xml php7.3-xmlrpc php7.3-curl php7.3-gd php7.3-imagick php7.3-cli php7.3-dev php7.3-imap php7.3-mbstring php7.3-opcache php7.3-soap php7.3-zip php7.3-intl -y # Removing the default index.html page and copying the project code RUN rm -f /var/www/html/index.html COPY . /var/www/html/ # Install ufw RUN apt install ufw -y RUN ufw app list # install library RUN apt-get install libapache2-mod-php7.3 # install additional packages RUN a2dismod mpm_event \u0026\u0026 a2enmod mpm_prefork \u0026\u0026 a2enmod php7.3 # Restart apache RUN service apache2 restart # Provide executable permissions to the code RUN chmod -R 0777 /var/www/html/* RUN chmod -R 0777 /var/* # Change WORKDIR WORKDIR /var/www/html CMD [\"apachectl\",\"-D\",\"FOREGROUND\"] RUN a2enmod rewrite EXPOSE 80 EXPOSE 443 In this dockerfile I have installed php 7.3 version which was required for my application. Use the below command to build the image from the dockerfile: docker build -f dockerfile-lamp-stack.dockerfile . Next let us create the containers to deploy the application. Below is the snippet of docker-compose file: version: '3' networks: lamp-stack-net: external: true volumes: mysql_storage_01: external: true services: lamp_stack: image: lamp_stack_app:v1 privileged: true build: context: path_to_code dockerfile: dockerfile-lamp-stack.dockerfile container_name: app_cont networks: - lamp-stack-net ports: - \"8010:80\" volumes: - path_to_code/:/var/www/html/ mysql_service: image: mysql:5.7.25 container_name: mysql_cont ports: - \"3306:3306\" environment: # MYSQL_ROOT_PASSWORD: '' # MYSQL_ALLOW_EMPTY_PASSWORD : 'yes' MYSQL_ROOT_PASSWORD: root MYSQL_DATABASE: test_db MYSQL_USER: test_user MYSQL_PASSWORD: test@123 networks: - lamp-stack-net restart: always volumes: - path_to_dump_file/:/home/ - mysql_storage_01:/var/lib/mysql In the above docker-compose file we are building the LAMP Stack container and MYSQL container. For the lamp_stack service we need to give the context of the code and place the dockerfile at that loaction in order to build our LAMP Stack image. We are exposing the port 8010 where the application will be served on the browser. Next is mysql_service, where we are creating the mysql container by using MYSQL 5.7 version. In the volumes section we have to use the location to our source code and database dump file respectively on line 25 and 43 respectively. Use the below commands to create network and volumes respectively: docker network create lamp-stack-net docker volume create --name=mysql_storage_01 Now let us create the containers using the below command: docker-compose -f docker-compose-lamp-stack.yml up -d Now check if both the containers are up and running using the below command: docker ps If both the containers are up and running then check on the browser using: \u003cIP\u003e:8010 or localhost:8010 This will serve the default page on the browser. To connect to the database you will need to use the database details in your php config file. Hope this article was helpful. Happy Learing!!! Vrinda Hegde is a DevOps Engineer, who likes to explore orchestration tools and automate the process of deploying containerized applications. She likes to share her findings by writing articles on medium.com. She can be reached out on LinkedIn or via email ","date":"2022-06-06","objectID":"/dockerizing_lamp_stack_app/:1:0","tags":["How-to","docker","blog","lamp stack","technology","VrindaHegde","VisitingAuthors"],"title":"Dockerizing LAMP Stack Application","uri":"/dockerizing_lamp_stack_app/"},{"categories":["How-to","technology"],"content":"Kubernetes won the race to become the de-facto container orchestration engine. Ever since, a lot of companies are coming up with their own version of container orchestration based on top of Kubernetes, while, some other companies are developing tools and products to complete the Kubernetes in the orchestration space. Having said that, the top three cloud provides, have come up with their own version of managed Kubernetes offerings: Microsoft Azure offers the Azure Kubernetes Service (AKS) AWS offers the Amazon Elastic Kubernetes Service (EKS) Google Cloud offers the Google Kubernetes Engine (GKE) ","date":"2022-06-03","objectID":"/amazoneks/:0:0","tags":["How-to","blog","aws","AmazonEKS","technology"],"title":"Amazon EKS: A managed Kubernetes service by AWS","uri":"/amazoneks/"},{"categories":["How-to","technology"],"content":"What is Amazon EKS? Amazon Elastic Kubernetes Service (Amazon EKS) is a managed service that you can use to run Kubernetes on AWS without needing to install, operate, and maintain your own Kubernetes control plane or nodes. Kubernetes is an open-source system for automating the deployment, scaling, and management of containerized applications. In June 2018, AWS made EKS generally available for all. EKS is similar to AKS and GKE as it supports swift development and deployment of applications based on Kubernetes. Let’s take a look at some of the critical areas to understand how EKS behaves as a managed Kubernetes service. ","date":"2022-06-03","objectID":"/amazoneks/:1:0","tags":["How-to","blog","aws","AmazonEKS","technology"],"title":"Amazon EKS: A managed Kubernetes service by AWS","uri":"/amazoneks/"},{"categories":["How-to","technology"],"content":"Updates Updating EKS requires multiple steps to be implemented. The users of EKS need to run instructions via command-line, which initiates the update. These steps are required to manage the updates for nodes. ","date":"2022-06-03","objectID":"/amazoneks/:2:0","tags":["How-to","blog","aws","AmazonEKS","technology"],"title":"Amazon EKS: A managed Kubernetes service by AWS","uri":"/amazoneks/"},{"categories":["How-to","technology"],"content":"Resource monitoring AWS offers lightweight monitoring for the control plane directly in Cloudwatch. To monitor the workers, you can use Kubernetes Container Insights Metrics provided via a specific CloudWatch agent you can install in the cluster. ","date":"2022-06-03","objectID":"/amazoneks/:3:0","tags":["How-to","blog","aws","AmazonEKS","technology"],"title":"Amazon EKS: A managed Kubernetes service by AWS","uri":"/amazoneks/"},{"categories":["How-to","technology"],"content":"Availability AWS has 66 availability zones. And the footprint will increase as Amazon plans to add 12 more zones to its total tally of AZs. ","date":"2022-06-03","objectID":"/amazoneks/:4:0","tags":["How-to","blog","aws","AmazonEKS","technology"],"title":"Amazon EKS: A managed Kubernetes service by AWS","uri":"/amazoneks/"},{"categories":["How-to","technology"],"content":"CLI Support The official user guide also uses the following command line tools: kubectl – A command line tool for working with Kubernetes clusters. eksctl – A command line tool for working with EKS clusters that automates many individual tasks. AWS CLI – A command line tool for working with AWS services, including Amazon EKS. Creating a cluster via eksctl is as easy as eksctl create cluster, no other parameters required. ","date":"2022-06-03","objectID":"/amazoneks/:5:0","tags":["How-to","blog","aws","AmazonEKS","technology"],"title":"Amazon EKS: A managed Kubernetes service by AWS","uri":"/amazoneks/"},{"categories":["How-to","technology"],"content":"Node Pools In a cluster of K8s, a group of nodes that share the same configuration are referred to as a node pool. Node pools are vital as they allow the cluster to function with different machines for various workloads. The users can designate the node pools with the service they want to deploy them with. EKS allows its users to run 100 nodes per node pool. ","date":"2022-06-03","objectID":"/amazoneks/:6:0","tags":["How-to","blog","aws","AmazonEKS","technology"],"title":"Amazon EKS: A managed Kubernetes service by AWS","uri":"/amazoneks/"},{"categories":["How-to","technology"],"content":"Auto-Scaling One of the standout features of Kubernetes is its seamless ability to scale the nodes. This enables the cluster to trim down on resources usage. This not only saves time but is also cost-effective as both heavy and lean demands are met with the right amount of resources. Additionally, Auto-scaling can be utilized to tweak the resource utilization plans for the present and future. EKS autoscaling is relatively easy as it only takes a few manual steps. Also, it is the only one to allow bare-metal nodes to run your Kubernetes cluster. ","date":"2022-06-03","objectID":"/amazoneks/:7:0","tags":["How-to","blog","aws","AmazonEKS","technology"],"title":"Amazon EKS: A managed Kubernetes service by AWS","uri":"/amazoneks/"},{"categories":["How-to","technology"],"content":"Pricing EKS bills 10 cents/hour/control plane. Additionally, USD 0.20 per hour is billed for every deployed cluster. AWS doesn’t allow the use of their free tier to test an EKS cluster is that EKS requires bigger machines than the tX.micro tier, and EKS hourly pricing is not in the free tier. Hope the article was useful, thanks for reading! ","date":"2022-06-03","objectID":"/amazoneks/:8:0","tags":["How-to","blog","aws","AmazonEKS","technology"],"title":"Amazon EKS: A managed Kubernetes service by AWS","uri":"/amazoneks/"},{"categories":["How-to","technology"],"content":" Nope! This is article is not about what AWS is, how it works and what are its services, rather it talks about AWS and how to get started with it being a complete beginner. I am working on AWS for more than 5+ years now and have taught professionally for 2 years, hence, I understand how overwhelming it might get for a beginner to figure out how to start studying AWS. For many, due to one or the other reasons, AWS is the first public cloud provider which they get exposed to. Hence, this article aims at providing you a detailed overview about AWS and its ecosystem and how to get started with it. I plan to write a similar blogpost on the other two major cloud providers, Google Cloud Platform [GCP] and Microsoft Azure, but that’s for another day once I myself get a good hold on them. This article is for a student, who is just starting with their cloud journey, or for a software developer, tester, technical sales who are new to AWS or cloud for that matter. If your team is looking to invest in AWS, or your company is undergoing digital transformation, or if you are a startup looking to migrate some/entire load on to AWS, then this article is for you. I hope this article serves you’ll well. Let’s get started! ","date":"2022-06-03","objectID":"/aws_for_beginners/:0:0","tags":["How-to","blog","aws","technology"],"title":"How to get started on AWS","uri":"/aws_for_beginners/"},{"categories":["How-to","technology"],"content":"What is AWS? AWS stands for Amazon Web Services. It offers variety of models, the most famous being pay-as-you-go, to work with the basic infrastructure you would need to run your company. The services include range of products in the compute, storage, networking, database, analytics and many other domain. It is a comprehensive cloud computing platform that includes infrastructure as a service (IaaS), platform as a service (PaaS) as well as sofware as a service (SaaS) offerings. services ","date":"2022-06-03","objectID":"/aws_for_beginners/:1:0","tags":["How-to","blog","aws","technology"],"title":"How to get started on AWS","uri":"/aws_for_beginners/"},{"categories":["How-to","technology"],"content":"Start with core services AWS offers more than 150+ services, which can be quite intimidating for a beginner. However, you don’t need to master them all. You can start with the core services which acts as a building blocks of any cloud provider and also help you get certified eventually. I always recommend to start with Identity and Access Management (IAM) service for couple of reasons. Since it is absolutely free of cost: You get a good grasp of AWS console while exploring and learning IAM Even if you mess up with IAM, your account most probably will not get charged Below are some of the key “building block” services which form the core of the AWS platform. Getting familiar with these is a good place to start your learning: Elastic Compute Cloud (EC2): virtual servers Relational Database Service (RDS): relational databases Elastic Block Store (EBS): block storage Simple Storage Service (S3): file storage Identity and Access Management (IAM): users, groups and roles Virtual Private Cloud (VPC): networking Remember Learn core AWS services first Prioritise hands-on learning Structure your learning ","date":"2022-06-03","objectID":"/aws_for_beginners/:2:0","tags":["How-to","blog","aws","technology"],"title":"How to get started on AWS","uri":"/aws_for_beginners/"},{"categories":["How-to","technology"],"content":"Certifications Getting certified on AWS have great benefits and you don’t have to get certified on all of them. Depending on your role/domain/aspirations, you select a specific track and prepare for accordingly. The good thing is, AWS certification exams are simplified and borken down into different categories. Associate: The associate exams are your more entry level exams, Professional: The professional exams build on top of the associate exams with more detail. Specialty: You can also go down a specialty route and learning a specific topic like Networking or Security. AWS Certifications The best place to start as a complete beginner is with the Cloud Practitioner exam. The Cloud Practitioner exam is going to give you a solid basis in AWS. When you’ve completed the Cloud Practitioner exam, you can then take a look at one of the associate exams, Architect, SysOps or Developer depending on your preference. ","date":"2022-06-03","objectID":"/aws_for_beginners/:3:0","tags":["How-to","blog","aws","technology"],"title":"How to get started on AWS","uri":"/aws_for_beginners/"},{"categories":["How-to","technology"],"content":"Resources Alright, since we have covered what AWS is, what are its services and what certifications AWS provides, I think it’s time to talk about the resources to master those services. Documentation youtube stackoverflow aws.amazon.com/free In addition to these resources, also checkout: • Whitepapers: Resources designed to broaden your technical understanding, written by the AWS team, independent analysts, and AWS partners. • FAQ: Commonly raised issues and questions that will help you understand AWS products, services, and features beyond the scope of your personal experience. ","date":"2022-06-03","objectID":"/aws_for_beginners/:4:0","tags":["How-to","blog","aws","technology"],"title":"How to get started on AWS","uri":"/aws_for_beginners/"},{"categories":["How-to","technology"],"content":"Prerequisites Before you start with hands-on practice, you need: AWS free tier account to kick off journey Credit card or debit card is mandatory ( It will not charge if you are using the free tier resources properly. Do checkout the free tier limit for the services you are working with). ","date":"2022-06-03","objectID":"/aws_for_beginners/:5:0","tags":["How-to","blog","aws","technology"],"title":"How to get started on AWS","uri":"/aws_for_beginners/"},{"categories":["How-to","technology"],"content":"Events AWS hosts events, both online and in-person, bringing the cloud computing community together to connect, collaborate, and learn from AWS experts. These events ranges from AWS Summits, to partner events, webinars, training and certification and more. Check out the events page for more details. ","date":"2022-06-03","objectID":"/aws_for_beginners/:6:0","tags":["How-to","blog","aws","technology"],"title":"How to get started on AWS","uri":"/aws_for_beginners/"},{"categories":["How-to","technology"],"content":"Careers AWS offers exciting and variety of roles around the globe. Check out the careers page for more details. Note: AWS frequently keeps adding new services to its current pool and reguarly comes up with new certifications. Also, the links that I have shared in this articles may get updated. If you come across any such instance, feel free to reach out to me so that it will help me keep this article updated. Happy learning! ","date":"2022-06-03","objectID":"/aws_for_beginners/:7:0","tags":["How-to","blog","aws","technology"],"title":"How to get started on AWS","uri":"/aws_for_beginners/"},{"categories":["VisitingAuthors","How-to","technology"],"content":" AWS Lambda is a compute service that lets you run code without provisioning or managing servers. Lambda runs your code only when needed and scales automatically, from a few requests per day to thousands per second. AWS Lambda in our task is invoked using the API Gateway. Slack is a channel-based messaging platform. With Slack, people can work together more effectively, connect all their software tools and services, and find the information they need to do their best work — all within a secure, enterprise-grade environment. Invoke lambda function through API Gateway from Slack This blog covers on how to invoke AWS Lambda function through API Gateway when there is a new message posted to any Slack channel. Here are a few business cases where this can be used: If a Quality Analyst wants to trigger an integration test by sending a message via Slack. If a Developer wants to get logs of a Load Balancer which is managed by the Security Team. Here are the steps to be followed…. Step1: First let’s configure Slack Create a workspace in slack.com, then create a slack app at api.slack.com/apps in the newly created workspace and then go to OAuth \u0026 Permissions tab, add channels:history, channels:read, chat:write, im:history \u0026 mpim:history OAuth scope to Bot Token Scopes and User Token Scopes. Now install your app to workspace and allow necessary permissions, then User OAuth Token \u0026 Bot User OAuth Token are generated. Bot and User Token Scopes OAuth \u0026 Permissions page And also keep the copy of verification token which is present in the Basic Information tab of your app. Access Verification Token from Basic Information page Step 2: Now let’s move on to AWS Lambda configuration part Let’s create a Lambda function using Author from scratch lambda function template. Select runtime as Node.js and Create a new role with basic Lambda permissions. Creating Lambda function in AWS Lambda Console Now your lambda function is created. In the code tab, change index.js file as follows. //Verify Url - https://api.slack.com/events/url_verification function verify(data, callback) { if (data.token === VERIFICATION_TOKEN) callback(null, data.challenge); else callback(\"verification failed\"); } //Print the slack message on the console function process(data,context, callback) { console.log('context:', JSON.stringify(context)); console.log('data.event.text',JSON.stringify(data.event.text)); callback(null,data.event.message); } // Lambda handler exports.handler = (data, context, callback) =\u003e { console.log(data); switch (data.type) { case \"url_verification\": verify(data, callback); break; case \"event_callback\": process(data,context, callback);break; default: callback(null,\"Hello from Lambda\"); } }; [APP VERIFICATION TOKEN]: Access this token in the basic information tab of the slack app created in Step 1(Refer to Access Verification Token from Basic Information page figure in Step 1). [SLACK ACCESS TOKEN]: Access Bot User OAuth Token from OAuth Tokens for Your Team in OAuth \u0026 Permissions page(Refer to OAuth \u0026 Permissions page figure in Step 1). Step 3: In this step, API Gateway is configured Add a REST API gateway trigger to the lambda function and select Security as Open. In the additional settings, change the deployment stage name to prod(Optional). After the trigger is added, go to API gateway console, add method POST to the resource. After creating POST method, then select Deploy API option from the Actions menu. Add POST method to the SlackLambda resource Select Deploy API Select the Deployment stage which was created in the earlier step, then click deploy Deploy API to prod stage Step 4: Slack Configuration post AWS Lambda/API Gateway configuration From Slack App(api.slack.com/apps), select the newly created app. Then go to Event Subscriptions and Enable Events. In the Request URL textbox, paste the API gateway URL which can be accessed from the configuration tab of Lambda function. Enabling EVents in Event Subscription page Proceed once the URL is verified. S","date":"2022-06-03","objectID":"/lambda_slack/:0:0","tags":["How-to","aws","blog","aws lambda","slack","technology","SiyaAmonkar","VarshareddyKumbham","VisitingAuthors"],"title":"Invoke AWS Lambda from Slack","uri":"/lambda_slack/"},{"categories":["VisitingAuthors","How-to","technology"],"content":"Now let’s test it! Open slack.com and Launch the workspace where the app is created then post the message to a slack channel or a direct message. Message posted to slack channel Now check the message and metadata of the message in the cloud-watch log console. To conclude, When someone posts a message to a slack channel, the API gateway triggers the lambda function and cloud watch logs has a record of all the Lambda invocations. We have come to an end of this blog. Hope you liked it, please do share it with your friends. Happy Reading! Siya Amonkar is a DevOps Engineer who likes to explore new tools and technologies. She can be reached out on LinkedIn or via email Varsha is a software engineer with overall experience of 2 years, who loves to play with devops tools and is a cloud computing enthusiast. She can be reached out on LinkedIn or via email ","date":"2022-06-03","objectID":"/lambda_slack/:1:0","tags":["How-to","aws","blog","aws lambda","slack","technology","SiyaAmonkar","VarshareddyKumbham","VisitingAuthors"],"title":"Invoke AWS Lambda from Slack","uri":"/lambda_slack/"},{"categories":["How-to","VisitingAuthors","technology"],"content":"Amazon S3 is used to store any amount of data of object type such as static websites, documents, images, videos and backup files as well. This data can be retrieved at any time from anywhere on the web. Inside S3 you create Buckets to store your objects/data. You can create multiple buckets in any region and it stores data upto 5TB. After you create buckets and upload objects in Amazon S3, you can manage your object storage using below features: Versioning: You can create multiple copies of your data. If the data center goes down in any region your data will be maintained in other data center of that region. If your data gets deleted versioning helps in retrieving it as it creates version ID of your data. It prevents overwriting or accidential deletion of data. As and when you make any changes in your data the latest copy will be available on the top. Cross region replication: If you want to copy data from one region bucket to another region bucket you can do it using cross region replication. Transfer acceleration: Transferring the data from one location to another location with low latency. Transfer acceleration puts data to CloudFront Edge Location and transfers your data. Amazon CloudFront is been explained in another blog. Lifecyle: You can decide the lifecyle of an object by just settings its lifecycle. For example: Now if your objects are in Standard-IA and after few days you want those objects to be in Glacier in that case you can set lifecycle to your objects. There are classes of S3 to store the objects based on the requirements: S3 Standard S3 Infrequent Access or Standard-IA S3 Glacier ","date":"2022-05-01","objectID":"/s3/:0:0","tags":["How-to","technology","Amazon S3","aws","ShreyaDhange","VisitingAuthors"],"title":"Amazon S3 at a glance","uri":"/s3/"},{"categories":["How-to","VisitingAuthors","technology"],"content":"S3 Standard Suppose you have hosted a website and its objects are stored in Amazon S3 out of which you only need few data to be fetched frequently. You can use S3 Standard for such objects as its durability is 99.999% and can be fetched in millisec and is highly available. You can store any file that can be as low as KB. You can store the data for an indefinite period of time. ","date":"2022-05-01","objectID":"/s3/:1:0","tags":["How-to","technology","Amazon S3","aws","ShreyaDhange","VisitingAuthors"],"title":"Amazon S3 at a glance","uri":"/s3/"},{"categories":["How-to","VisitingAuthors","technology"],"content":"Key features: Low latency and high performance. Durability is 99.999999999% of objects across multiple Availability Zones Supports SSL for data in transit and encryption of data at rest ","date":"2022-05-01","objectID":"/s3/:1:1","tags":["How-to","technology","Amazon S3","aws","ShreyaDhange","VisitingAuthors"],"title":"Amazon S3 at a glance","uri":"/s3/"},{"categories":["How-to","VisitingAuthors","technology"],"content":"S3 Infrequent Access or Standard-IA: Suppose you have a image gallery of 2020 images and you do not need it frequently but you only need few images as and when needed so you stored such datas in Standard-IA. Standard-IA fetches data at a durability of 99.9% and can store the data uptill 30 days with a low per GB storage price and per GB retrieval charge. ","date":"2022-05-01","objectID":"/s3/:2:0","tags":["How-to","technology","Amazon S3","aws","ShreyaDhange","VisitingAuthors"],"title":"Amazon S3 at a glance","uri":"/s3/"},{"categories":["How-to","VisitingAuthors","technology"],"content":"Key features: Low latency and high performance. Durability is 99.999999999% of objects across multiple Availability Zones Supports SSL for data in transit and encryption of data at rest ","date":"2022-05-01","objectID":"/s3/:2:1","tags":["How-to","technology","Amazon S3","aws","ShreyaDhange","VisitingAuthors"],"title":"Amazon S3 at a glance","uri":"/s3/"},{"categories":["How-to","VisitingAuthors","technology"],"content":"S3 Glacier: S3 Glacier stores archival data that you do not require it on daily bases for example: medical images, news media assets, or genomics data. Its durability of fetching the data is milliseconds to hours to fit your performance needs. You can increase its fetching time by paying certain price per GB pay. It can store 128 KB minimum object size. ","date":"2022-05-01","objectID":"/s3/:3:0","tags":["How-to","technology","Amazon S3","aws","ShreyaDhange","VisitingAuthors"],"title":"Amazon S3 at a glance","uri":"/s3/"},{"categories":["How-to","VisitingAuthors","technology"],"content":"Key features: Low latency and high performance. Data retrieval options from milliseconds to hours as required. Durability is 99.999999999% of objects across multiple Availability Zones Supports SSL for data in transit and encryption of data at rest Data is resilient in the event of the destruction of one entire Availability Zone Shreya Dhange is a Technical Training Developer at Red Hat, who likes to explore and learn new technologies and share her knowledge by writing articles. She has completed her Masters in Computer Science and has gained award for her exemplary academic performance. She has been engaged in creating and delivering content in the cloud and linux space. She can be reached out LinkedIn or via email. ","date":"2022-05-01","objectID":"/s3/:3:1","tags":["How-to","technology","Amazon S3","aws","ShreyaDhange","VisitingAuthors"],"title":"Amazon S3 at a glance","uri":"/s3/"},{"categories":["How-to","VisitingAuthors","technology"],"content":"In the previous article, we learned what VPC is, how does it work and what are its benefits. AWS provides a pre-configured to launch our instances, however, the best practice is to create your own custom VPC per your requirements and organization’s network and security policies and launch your instances in the custom VPC. In this article, we will discuss how to configure your own custom VPC. Leaving services running in cloud computing can escalate your bill quickly, hence, it is equally important to understand how to delete the resources once you are done using it. We will also see how to delete the VPC and related resources. ","date":"2022-04-18","objectID":"/configuring_vpc/:0:0","tags":["How-to","technology","aws","VPC","networking","ShreyaDhange","VisitingAuthors"],"title":"Configuring a custom VPC","uri":"/configuring_vpc/"},{"categories":["How-to","VisitingAuthors","technology"],"content":"Steps to configure VPC Go to AWS services from the dashboard in that Network \u0026 Content delivery select VPC. Create a VPC. Every VPC has one default VPC. Note: do not delete the default VPC. Here i have given the name as MyFirstVPC with the CIDR value as 10.0.0.0/16. You can choose any CIDR value of your choice between 10.0.0.0/16 - 10.0.0.0/28. Now VPC is created successfully. Now if you will go the Route Tables on the left side of the dashboard you will see our default route table is been created. To confirm which one is our Route table check the VPC ID. You can give the name to it after confirming. I have given it as MyRouteTable. Inside our VPC we are going to create our subnets i.e. public subnet and private subnet. Give the name as PublicSubnet and PrivateSubnet and the VPC can be chosen from the VPC that you have created. Here i have created it as MyFirstVPC. Availability zone can be chosen any of your choice. CIDR block should be chosen according to the IP range that you have given. Here i give it as 10.0.1.0/24 for PrivateSubnet and 10.0.2.0/24 for PublicSubnet. Now the subnets are been created successfully. As we know all the new entries of the subnets will enter into our default Route table. Go to the Route table in that select Subnet Associates you will see your subnets. Note: Here we don’t have separate Route table all will enter into our default Route table. Now we will create separate Route table in our VPC i.e. Public Route table with the name PublicRT and Private Route table with the name PrivateRT. You can give the name of your choice. Now by default all my subnets are into the default Route table, now we will put our subnets into their respective Route table i.e. PublicSubnet will go to PublicRT and PrivateSubnet will go to PrivateRT. Note: we will not have any duplicate records i.e. 1 subnet can have any 1 Route table. Go to PublicRT in that select Subnet Associations and select Edit subnet associations and select the PublicSubnet and save it. Do the same for PrivateRT. Now if you will go to the the default Route table i.e. MyRouteTable there will be no subnets. Now we will create an Internet Gateway to connect to our VPC. Go to the Internet Gateway on the left side of the dashboard and create it. Here i gave the name as MyGateway you can give of your own choice. Now Internet Gateway is created successfully. Now go to Actions on the right side of your console and Attach the VPC. Now your Internet Gateway is attached to your VPC. Now will attach our Internet Gateway to our public route table which will be accessible through the internet. Now go to Route Tables in that select PublicRT and select Route and select Edit routes. Add Route and give any IP, here i give 0.0.0.0/0 i.e. from anywhere i can access my VPC. Now we will launch our instances into our subnets. Here, we will launch EC2 instance for PublicSubnet. Go to EC2 and select any instance. After selecting go to Configure Instance and select the Network i.e. the VPC you have created. Since we had given my CIDR value as 24 i will get 251 IPs. And we will enable Public IP. After that we will add Security Groups i.e. HTTP and HTTPS. Similarly launch an EC2 instance for PrivateSubnet and the process is same only the change will in Networks, here we will select PrivateSubnet and Public IP will be disabled for this. Now IPs of these instances will be given the range that you had given. Now go and check if my PublicInstance is accessible through the internet and similiarly we will do it for PrivateInstance. If you see that PublicInstance will have public ip wherein PrivateInstance will not have public ip which means only PublicInstance can be accessed through internet. By taking the remote access of the PublicInstance we can check if it is accessible. Since it is accessible, now go and remove the Internet Gateway from the PublicRT and check if it is accessible. PrivateInstance is not accessible through the internet. To access this you will go through the Publi","date":"2022-04-18","objectID":"/configuring_vpc/:1:0","tags":["How-to","technology","aws","VPC","networking","ShreyaDhange","VisitingAuthors"],"title":"Configuring a custom VPC","uri":"/configuring_vpc/"},{"categories":["How-to","VisitingAuthors","technology"],"content":"Steps to delete the VPC Start by terminating the instances. Navigate to Route Tables and remove the entries in the Route Table by going into the Edit routes and remove the NAT Gateway and delete the subnets and then remove public and private route table. Navigate to the Internet Gateway and detach it from VPC. Navigate to the NAT Gateway and delete it. Navigate to the Subnets and remove the entries. Navigate to VPC and delete the VPC. We hope you find this article helpful. Happy learning!👍🏻 Shreya Dhange is a Technical Training Developer at Red Hat, who likes to explore and learn new technologies and share her knowledge by writing articles. She has completed her Masters in Computer Science and has gained award for her exemplary academic performance. She has been engaged in creating and delivering content in the cloud and linux space. She can be reached out LinkedIn or via email. Parth Goswami is an opensource enthusiasts and likes to talk about AWS and cloud computing. ","date":"2022-04-18","objectID":"/configuring_vpc/:2:0","tags":["How-to","technology","aws","VPC","networking","ShreyaDhange","VisitingAuthors"],"title":"Configuring a custom VPC","uri":"/configuring_vpc/"},{"categories":["How-to","VisitingAuthors","technology"],"content":" If you start from the bare metal layer and go right up until the container layer through virtualization, base OS and the container engine, you would notice that networking runs throughout the stack. Networking is the backbone of any infrastructure. Hence, understanding the networking concepts becomes very important and if we are talking about cloud computing, it becomes ever more important. In this article, I will be discussing how networking works in the cloud computing. For clarity, this article focuses on Virtual Private Cloud[VPC], a networking service provided by Amazon Web Services. Let’s dive into it! ","date":"2022-03-11","objectID":"/decoding_vpc/:0:0","tags":["How-to","technology","aws","VPC","networking","ShreyaDhange","VisitingAuthors"],"title":"How does VPC work?","uri":"/decoding_vpc/"},{"categories":["How-to","VisitingAuthors","technology"],"content":"What is VPC? VPC stands for Virtual Private Cloud and is one of the most fundamental and widely used AWS service which provides a virtual network dedicated for the AWS environments. It is logically isolated from the other virtual networks in the cloud and exists within a single region. VPC allows for more control of AWS Cloud Network with extra layer of security. ","date":"2022-03-11","objectID":"/decoding_vpc/:1:0","tags":["How-to","technology","aws","VPC","networking","ShreyaDhange","VisitingAuthors"],"title":"How does VPC work?","uri":"/decoding_vpc/"},{"categories":["How-to","VisitingAuthors","technology"],"content":"How does VPC works? VPC cannot talk directly to the internet. For that we need Internet Gateway to talk to the internet. Internet Gateway is used to allow resources (subnets) in your VPC to access Internet. VPC can have multiple subnets and these subnets can be public facing or private. Subnets are the the spaces where your applications are running. Subnets are been decided by CIDR( Classless inter-domain routing). Public subnets are the ones that are accessible to the internet and Private subnets are the ones that are not accessible by the internet. For example: Amazon Shopping Website, you cannot access the database of this website but you can access the application of Amazon Shopping Website . This means that the application is running on public subnet and the database is running on private subnet. We decide these public and private subnets using Route Table. Route table is the place where you put all the entries of these public and private subnets i.e. IP address. Every VPC has one default Route table. All subnets that you launch will be a part of this Route table. Everything will go into the Route table i.e. public subnet, private subnet and the internet gateway which means anyone who is coming can access my private subnet which is dangerous. To over come this we use different Route tables i.e. we will make different public route table and different private route table i.e. the internet gateway can access my public subnet and not the private subnet. All the new entries of the subnet will enter into default Route table and whenever you want you can move these entries into public or private subnets. Internet gateway can access the public subnet and public subnet will communicate with the private subnet. If the private subnet wants to talk to the internet but it cannot communicate directly to the internet that time we will use NAT. NAT, Network Address Translation, refers to the proxy server. Private subnet will talk to the internet through NAT but internet will not talk directly to the private subnet, it will first communicate with public subnet and then public subnet will talk to the private subnet. VPC reference diagram ","date":"2022-03-11","objectID":"/decoding_vpc/:2:0","tags":["How-to","technology","aws","VPC","networking","ShreyaDhange","VisitingAuthors"],"title":"How does VPC work?","uri":"/decoding_vpc/"},{"categories":["How-to","VisitingAuthors","technology"],"content":"What is CIDR? CIDR is Classless inter-domain routing. This will decide how many subnets you want to launch in a machine (VPC). Inside these subnets we can launch our machines. It contains the range of IPs i.e. IPv4 and IPv6. IPv6 - 128 bits IPv4 - 32 bits x.x.x.x/16 - x.x.x.x/28 (choose any value between this CIDR range) Suppose, 10.0.0.0/16 = 32-16=16 ==\u003e 2^16= 65536 (65536 IP can be launched in 1 VPC of 16 CIDR value) 10.0.0.0/24 = 32-24= 8 ==\u003e 2^8= 256 10.0.0.0/28= 32-28= 4 ==\u003e 2^4= 16 A minimum 16 subnets and maximum 65536 subnets can be launched in 1 VPC. If your requirements is for more subnets then choose lower CIDR value and if requirement is less then choose higher CIDR value. ","date":"2022-03-11","objectID":"/decoding_vpc/:3:0","tags":["How-to","technology","aws","VPC","networking","ShreyaDhange","VisitingAuthors"],"title":"How does VPC work?","uri":"/decoding_vpc/"},{"categories":["How-to","VisitingAuthors","technology"],"content":"Benefits of VPC Define custom networks. Assigns static private ipv4 addresses to the instances. Define network interfaces and attach one or more network interface to the instance. Define routing between different subnets. Define internet access for the subnets. Define your network security by allowing/denying the traffic. In this article, we discussed what VPC is and how does it work along with some of its benefits. In the next article, I will be discussing how to configure VPC in AWS and the steps to delete it once we are done tinkering with it. Check out how to configure your custom VPC in this article. Shreya Dhange is a Technical Training Developer at Red Hat, who likes to explore and learn new technologies and share her knowledge by writing articles. She has completed her Masters in Computer Science and has gained award for her exemplary academic performance. She has been engaged in creating and delivering content in the cloud and linux space. She can be reached out LinkedIn or via email. Parth Goswami is an opensource enthusiasts and likes to talk about AWS and cloud computing. ","date":"2022-03-11","objectID":"/decoding_vpc/:4:0","tags":["How-to","technology","aws","VPC","networking","ShreyaDhange","VisitingAuthors"],"title":"How does VPC work?","uri":"/decoding_vpc/"},{"categories":["How-to","VisitingAuthors","technology"],"content":" In this article, we will see what AWS CloudFront is and in what scenario it is used. We will also take a look on some of its advantages and what edge locations are. Let’s consider an example where the user is searching for a website suppose www.primevideo.com and their request and response time should experience least latency. Now, if the amazon server is in the US and the user accessing prime video website is from California so they will experience less latency i.e 2 secs. This is due to less geographical distance between the system generating the request packet and server responding to the request Now if the user is from APAC (say India) and wants to access the same website, they are more likely to experience increased latency, assume 5 secs. To solve this problem Amazon has introduced CloudFront service which works on the principal of CDN (Content Delivery Network). CloudFront leverages the edge locations nearest to your system generating the request packet, which helps in content delivery. Basically it will cache your data and stores the data so that the other users, from the same geographical region as that of primary user, requesting for the same website will have less latency. It is cost effective and data sync is reliable. Edge location will fetch the data faster because it has high bandwidth. Understand the difference that Availability Zones [AZ] are set of data centers that hosts servers, websites, applications, analytics, data processing etc, while Edge Locations [EL] are the data center that primarily are used to cache the data and provide a better user experience by reducing the latency. In simple terms, AWS CloudFront is a fast CDN service that securely delivers data with low latency and high speed transfer of data. You can create CloudFront in any specific region which helps keeping persistent connection with the origin. ","date":"2021-11-21","objectID":"/cloudfront/:0:0","tags":["How-to","technology","aws","aws cloudfront","ShreyaDhange","VisitingAuthors"],"title":"How AWS CloudFront works?","uri":"/cloudfront/"},{"categories":["How-to","VisitingAuthors","technology"],"content":"How Edge location works? As we discussed, edge locations are the data centers that host the web content. Edge location will transfer less requested data to regional edge location. If regional edge location has has deleted it then edge location will request it from the origin. As of writing this article, to deliver content to end users with lower latency, Amazon CloudFront uses a global network of 410+ Points of Presence (400+ Edge locations and 13 regional mid-tier caches) in 90+ cities across 47 countries. ","date":"2021-11-21","objectID":"/cloudfront/:1:0","tags":["How-to","technology","aws","aws cloudfront","ShreyaDhange","VisitingAuthors"],"title":"How AWS CloudFront works?","uri":"/cloudfront/"},{"categories":["How-to","VisitingAuthors","technology"],"content":"Advantages: Reduces the latency It is cost effective. Securely transfers data at fastest speed. High availability and scalability. Shows real time metrics and logging. ","date":"2021-11-21","objectID":"/cloudfront/:2:0","tags":["How-to","technology","aws","aws cloudfront","ShreyaDhange","VisitingAuthors"],"title":"How AWS CloudFront works?","uri":"/cloudfront/"},{"categories":["How-to","VisitingAuthors","technology"],"content":"How AWS leverage CloudFront for its own use? Transfer Acceleration takes advantage of the globally distributed edge locations in Amazon CloudFront. As the data arrives at an edge location, the data is routed to Amazon S3 over an optimized network path. Stay tuned to learn more about its configuration in the next blog Shreya Dhange is a Technical Training Developer at Red Hat, who likes to explore and learn new technologies and share her knowledge by writing articles. She has completed her Masters in Computer Science and has gained award for her exemplary academic performance. She has been engaged in creating and delivering content in the cloud and linux space. She can be reached out LinkedIn or via email. Parth Goswami is an opensource enthusiasts and likes to talk about AWS and cloud computing. ","date":"2021-11-21","objectID":"/cloudfront/:3:0","tags":["How-to","technology","aws","aws cloudfront","ShreyaDhange","VisitingAuthors"],"title":"How AWS CloudFront works?","uri":"/cloudfront/"},{"categories":["How-to","VisitingAuthors","technology"],"content":"Consider a scenerio: We have setup a VPC for the instances which are in the same range. The Public instance is having an internet connection and Private instance does not having an internet connection. Inside Public instance we have S3 bucket endpoint to which users will have the access to upload/download images and videos. The EC2 instances have an ELB (Elastic Load Balancer) attached with an Alarm set to it. The EC2 instances will have IAM roles attached to it having roles set like AWSS3FullAccess, AWSRekognitionFullAccess and AWSElasticTranscoderFullAccess. ","date":"2021-09-14","objectID":"/aws_rekognition/:1:0","tags":["How-to","technology","aws","Amazon S3","aws rekognition","aws transcoder","ShreyaDhange","VisitingAuthors"],"title":"How to configure AWS Rekognition and AWS Transcoder","uri":"/aws_rekognition/"},{"categories":["How-to","VisitingAuthors","technology"],"content":"Steps: Private instance will have NAT connectivity with Public instance which will open a browser with the endpoint given to the user to access S3 bucket uploads an image an request for image recognition. Inside EC2 run this command to get the output: aws rekognition detect-labels --image \"{\\\"S3Object\\\":{\\\"Bucket\\\":\\\"bucketname\\\",\\\"Name\\\":\\\"image.png\\\"}}\" --region us-east-1 The output of the image recognition will be stored in Amazon RDS. If the user wants a video to be converted to the version which can be accessible via phone or pc can upload the video in S3 Bucket and by setting up a pipeline in AWS Transcoder the user will be notified about the process completion and the user can further request for image/video recognition. The user having internet connection will access S3 Bucket with the endpoint provided and can further request for the AWS Rekognition. Procedure is same as above. ","date":"2021-09-14","objectID":"/aws_rekognition/:2:0","tags":["How-to","technology","aws","Amazon S3","aws rekognition","aws transcoder","ShreyaDhange","VisitingAuthors"],"title":"How to configure AWS Rekognition and AWS Transcoder","uri":"/aws_rekognition/"},{"categories":["How-to","VisitingAuthors","technology"],"content":"Configuration: First step is to configure VPC: 1.1 Create VPC with the name MyVPC: 1.2 Create 3 Subnets with the name Public, Public1, Private: 1.3 Create a separate Route table with names PublicRT and PrivateRT and place the subnets with the respective route table: 1.4 Create a Gateway with the name MyGateway: Create three EC2 instances and attach them with MyVPC as given below : Create IAM roles the instances: 3.1 Create IAM roles for Public Instance with respective policy as shown below: 3.2 Create IAM role for Private instance which holds RDS policy (optional) : Create S3 bucket as follows: 4.1 Create 2 buckets with the name inputbucket12 and outputbucket12: 4.2 The inputbucket12 will be used to upload images and videos: Setup for AWS Image Rekognition in Public instance using cli mode: Setting up AWS Transcoder for video converter: 6.1 Create Pipeline as given below: 6.2 Create job for the Pipeline: 6.3 Output of the video will be shown in outputbucket12: Shreya Dhange is a Technical Training Developer at Red Hat, who likes to explore and learn new technologies and share her knowledge by writing articles. She has completed her Masters in Computer Science and has gained award for her exemplary academic performance. She has been engaged in creating and delivering content in the cloud and linux space. She can be reached out LinkedIn or via email. ","date":"2021-09-14","objectID":"/aws_rekognition/:3:0","tags":["How-to","technology","aws","Amazon S3","aws rekognition","aws transcoder","ShreyaDhange","VisitingAuthors"],"title":"How to configure AWS Rekognition and AWS Transcoder","uri":"/aws_rekognition/"}]