[{"categories":["How-to","technology"],"content":"I started exploring and using Jupyter Notebook earlier this year for a project based on opensource enablement, licensing and operate first. This is the first time ever I was working with a notebook and apparently had to start from sratch right from understanding how it works, how it needs to be installed and finally how it is to be pushed to a repo and publish the work. In the process of exploring the tool, I went throught quite a handful of articles and videos. Everyone’s way was a bit different than the other and there was no way correct or proper to work around this. Amongst all, I found the below way to be the most effective and clean and hence thought of penning down the learnings create a how-to guide on installing and working with Jupyter Notebooks. This article is written based on a MacOS. Haven’t tried on a linux machine. Jupyter Notebook = JN //for ease of writing Prerequisites: Install python3 package ","date":"2022-07-21","objectID":"/jupyternotebooks/:0:0","tags":["JupyterNotebooks","How-to"],"title":"Getting started with Jupyter Notebooks","uri":"/jupyternotebooks/"},{"categories":["How-to","technology"],"content":"Install and launch JN First, let’s create a virtual environment to install JN parth@mac Desktop % python3 -m venv jupyter parth@mac Desktop % ls jupyter This creates a folder called jupyter in the current dir which has a python virtual env. To turn the venv on: parth@mac Desktop % source ~/jupyter/bin/activate (jupyter) parth@mac Desktop % The jupyter in parenthesis lets us know that we are in our python jupyter virtual environment. NOTE: This needs to be turned everytime you need to work with jupyter notebooks. Now, let’s install the jupyter notebook pkg (jupyter) parth@mac Desktop % pip3 install jupyter notebook This installs the jupyter and notebook packages. Creating another dir where I will be launching the JN from and where I can create pages. (jupyter) parth@mac Desktop % mkdir JupyterNotebooks (jupyter) parth@mac Desktop % cd JupyterNotebooks (jupyter) parth@mac JupyterNotebooks % pwd /Users/parth/Desktop/JupyterNotebooks Launching JN now (jupyter) parth@mac JupyterNotebooks % jupyter notebook . This runs a kernel process on the terminal and launches JN from the JupyterNotebooks dir in your default web browser. Note All and any work you do can be accessed via localhost and is saved locally. Click on the new button in the brower’s JN page to create a new page. The page can be a simple text file, a python file with .ipynb extension or markdown page with .md extention. Any pages you create are now saved in JupyterNotebooks dir. Alright, we have installed and launched JN to create the pages. Now it’s time we create a bundled book out of the pages and upload it in a git repo and publish the book. ","date":"2022-07-21","objectID":"/jupyternotebooks/:1:0","tags":["JupyterNotebooks","How-to"],"title":"Getting started with Jupyter Notebooks","uri":"/jupyternotebooks/"},{"categories":["How-to","technology"],"content":"Build and Publish the book Follow the steps mentioned here https://jupyterbook.org/en/stable/start/overview.html Install Jupyter Book pip3 install -U jupyter-book Tip Jupyter Book comes bundled with a lightweight sample book to help you understand a book’s structure. Create a sample book by running the following command: $ jupyter-book create mynewbook/ The name mynewbook can be replaced with any other name and create option will create a skeleton of book for you. Edit/add/modify the pages under this book. Once you are done with creating pages and have configured _toc.yml and _config.yml, it’s time to build the book. $ jupyter-book build mynewbook/ Use the same build option to re-build the book if you make any new changes. The book is build. Once all the final reviews are done, it can be published in the git repository. Follow the link below to publish the book online. https://jupyterbook.org/en/stable/start/publish.html Checkout some of my works based on Jupyter Notebook Opensource Enablement Opensource Licensing Thank you for reading! ","date":"2022-07-21","objectID":"/jupyternotebooks/:2:0","tags":["JupyterNotebooks","How-to"],"title":"Getting started with Jupyter Notebooks","uri":"/jupyternotebooks/"},{"categories":["community","UnconventionalContributors"],"content":"We have many diverse contributors in opensource that help upstream communities in unconventional ways, which doesn’t require any coding or development skills. Starting with a non-code contribution can help anyone overcome the sense of failure and not being good enough, and it can also serve as a springboard for our open source adventure. This interview series aims to highlight some non-code open source contributions that anyone can make right now to get started contributing. For this month’s edition, we talked about Mozilla and how a new contributor can get started with their first contributions, with Priyanka Pandey. Can you tell our readers a little bit about yourself, your experience at Red Hat, and what you do now? I was introduced to the world of Open Source back in my college days. My first Open Source contribution was translating Firefox strings from English to my native language, Bengali. During the span of my college life, I had the opportunity to volunteer with a few different communities and meet some very interesting and inspiring people from the Open Source world. It was during a few of these interactions that I happened to meet some Red Hatters, who were extremely passionate about Open Source. Red Hat, a for-profit organization, was generating revenue year-on-year and still contained the soul of Open Source at its heart; it was rare and impressive. Red Hat became my dream from that moment forward!The tiny adult that I was back then, started aspiring to be a Red Hatter. The dream turned into reality soon when I noticed an opening for a Technical Writer position at the Red Hat Pune office, back in 2015. I joined Red Hat as an Associate Technical Writer. Before joining Red Hat as an employee, I had visited the Red Hat office a couple of times for different community events. Whenever a visitor accesses any of the Red Hat office premises, they are given a temporary visitor pass. My friends still tease me, saying that I was using so many of those temporary passes that the organization got tired and decided to give me a permanent employee ID card. Well, I am definitely not complaining! :P Achieving the dream was only the start of a beautiful journey, a journey that is still ongoing. For the initial four years at Red Hat, I did write technical documents for the middleware product line of Red Hat, mainly EAP. As a writer, we were seated along with the technical support engineers of the product and would be collaborating with them very closely to understand customer requirements. During these collaborations, I realized that the opportunity to directly interact with customers everyday, to be able to help resolve their technical issues, had a different level of work satisfaction. When I shared my thoughts with my writing team’s manager, I was surprised to see the level of support and mentoring that he provided which helped me land an internal opening within the Technical Support team of Red Hat. In 2019, I took up the role of a Technical Lead (aka, Support Operations Lead) within the RHEL support team. From there began another phase of the journey. Being on our toes for all days, 24x7; handling customer escalation; firefighting difficult customer crisis situations, the more challenges that came in, the more my enthusiasm and energy grew. In July of 2020, I became an Associate Manager within the RHEL support team and exactly 2 years later and around 15 days back, I got promoted to be a Manager for the same team now. Well, congratulations on your promotion in that case. While visiting the Red Hat office for community events, I am sure you wouldn’t have ever thought of becoming a manager at Red Hat. On that note, could you tell me about the product(s) you’ve contributed to so far? When it comes to open source contributions, I was primarily involved with the Mozilla community. From translating Firefox strings to fixing simpler coding bugs; from evangelizing Mozilla products at different forums to writing documents for the Mozill","date":"2022-07-20","objectID":"/contributorblog_2/:0:0","tags":["blogs","community","mozilla","wikimedia","outreachy"],"title":"Contributor Blog #2: A journey from being a passive listener to an active contributor","uri":"/contributorblog_2/"},{"categories":["How-to","VisitingAuthors","technology"],"content":"Recently, while working on an internal project I was required to establish network connection between a Windows server and a RHEL server. Sounds easy, right? It would have been indeed, however, the task was not only to establish the network but also to ping the Windows server from RHEL via ansible, and that’s where the scenario got tricky. Also, the servers were EC2 instances hosted on AWS. I went through a few articles and videos on and thought of documenting my learnings in this blog. Amazon Elastic Compute Cloud (Amazon EC2) provides scalable computing capacity in the Amazon Web Services (AWS) Cloud. Using Amazon EC2 eliminates your need to invest in hardware up front, so you can develop and deploy applications faster. You can use Amazon EC2 to launch as many or as few virtual servers as you need, configure security and networking, and manage storage. Amazon EC2 enables you to scale up or down to handle changes in requirements or spikes in popularity, reducing your need to forecast traffic. Ping is a command-line utility, available on virtually any operating system with network connectivity, that acts as a test to see if a networked device is reachable. The ping command sends a request over the network to a specific device. A successful ping results in a response from the computer that was pinged back to the originating computer. Let’s start by getting to know the host requirements. Host requirements: For Ansible to communicate to a Windows host and use Windows modules, the Windows host must meet these requirements: Ansible can generally manage Windows versions under current and extended support from Microsoft. Ansible can manage desktop OSs including Windows 7, 8.1, and 10, and server OSs including Windows Server 2008, 2008 R2, 2012, 2012 R2, 2016, and 2019. Ansible requires PowerShell 3.0 or newer and at least .NET 4.0 to be installed on the Windows host. A WinRM listener should be created and activated. Use below steps to configure: ","date":"2022-07-11","objectID":"/ansible_sd/:0:0","tags":["How-to","technology","aws","windows","Red Hat","RHEL","ansible","ShreyaDhange","VisitingAuthors"],"title":"Managing Windows Server from Red Hat Server using Ansible on AWS EC2","uri":"/ansible_sd/"},{"categories":["How-to","VisitingAuthors","technology"],"content":"1. Configure Windows server 2016: a. Open Windows PowerShell and check the version: PS C:\\Users\\Adminstrator\u003e Get-Host | Select-Object version powershell version The powershell version should at least be 3.0 or more. If not then upgrade it using this document. Since we have version 5.1 no need to upgrade the version. b. Once PowerShell has been upgraded, the final step is for the WinRM service to be configured so that Ansible can connect to it: PS C:\\Users\\Administrator\u003e [Net.ServicePointManager]::SecurityProtocol = [Net.SecurityProtocolType]::Tls12 PS C:\\Users\\Administrator\u003e $url = \"https://raw.githubusercontent.com/ansible/ansible/devel/examples/scripts/ConfigureRemotingForAnsible.ps1\"$file = \"$env:temp\\ConfigureRemotingForAnsible.ps1\" PS C:\\Users\\Administrator\u003e (New-Object -TypeName System.Net.WebClient).DownloadFile($url, $file) PS C:\\Users\\Administrator\u003e powershell.exe -ExecutionPolicy ByPass -File $file c. Run this below script on Windows PowerShell ISE and check the version after successful script completion: Param([string]$computerName) Function enableWinRM { $result = winrm id -r:$global:compName 2\u003e$null Write-Host if ($LastExitCode -eq 0) { Write-Host \"WinRM already enabled on\" $global:compName \"...\" -ForegroundColor green } else { Write-Host \"Enabling WinRM on\" $global:compName \"...\" -ForegroundColor red .\\pstools\\psexec.exe \\\\$global:compName -s C:\\Windows\\system32\\winrm.cmd qc -quiet if ($LastExitCode -eq 0) { .\\pstools\\psservice.exe \\\\$global:compName restart WinRM $result = winrm id -r:$global:compName 2\u003e$null if ($LastExitCode -eq 0) {Write-Host 'WinRM successfully enabled!' -ForegroundColor green} else {exit 1} } else {exit 1} } } $global:compName = $computerName enableWinRM exit 0 version after script completion d. Check if ports are listening: PS C:\\Users\\Administrator\u003e winrm enumerate winrm/config/Listener port listener ","date":"2022-07-11","objectID":"/ansible_sd/:1:0","tags":["How-to","technology","aws","windows","Red Hat","RHEL","ansible","ShreyaDhange","VisitingAuthors"],"title":"Managing Windows Server from Red Hat Server using Ansible on AWS EC2","uri":"/ansible_sd/"},{"categories":["How-to","VisitingAuthors","technology"],"content":"2. Configure Red Hat 8 Server: a. If you have subscription manager account then subscribe your system using subscription-manager command: [root@ip-172-31-23-177 ~]# subscription-manager register b. Install ansible if package is not available: [root@ip-172-31-23-177 ~]# yum install ansible c. Install python-pip package: [root@ip-172-31-23-177 ~]# pip2 --version [root@ip-172-31-23-177 ~]# pip3 --version [root@ip-172-31-23-177 ~]# dnf install python2-pip [root@ip-172-31-23-177 ~]# dnf install python3-pip [root@ip-172-31-23-177 ~]# pip2 --version pip 9.0.3 from /usr/lib/python2.7/site-packages (python 2.7) [root@ip-172-31-23-177 ~]# pip3 --version pip 9.0.3 from /usr/lib/python3.6/site-packages (python 3.6) [root@ip-172-31-23-177 ~]# pip3 install \"pywinrm\u003e=0.2.2\" d. Now write a ansible playbook to ping windows server: [root@ip-172-31-23-177 ~]# tail /etc/ansible/hosts ## db-[99:101]-node.example.com [windows] 107.20.75.188 [windows:vars] ansible_user=\"windows_username\" //for example: ansible_user=\"Administrator\" ansible_password=\"windows_user_password\" ansible_connection=winrm ansible_winrm_server_cert_validation=ignore e. Use the below command to ping windows server: [root@ip-172-31-23-177 ansible]# ansible all -i hosts -m win_ping pinging windows server ","date":"2022-07-11","objectID":"/ansible_sd/:2:0","tags":["How-to","technology","aws","windows","Red Hat","RHEL","ansible","ShreyaDhange","VisitingAuthors"],"title":"Managing Windows Server from Red Hat Server using Ansible on AWS EC2","uri":"/ansible_sd/"},{"categories":["How-to","VisitingAuthors","technology"],"content":"Resources: Setting up a Windows Host WinRM Setup WinRM setup script Linux and Windows host setup How to install pip in RHEL 8 / CentOS 8 step by step instructions Windows PowerShell Upgrade Ansible-windows-lab-setup (where ansible server is in linux and target node is in windows) Tip: RDP Port No: 3389 Shreya Dhange is a Technical Training Developer at Red Hat, who likes to explore and learn new technologies and share her knowledge by writing articles. She has completed her Masters in Computer Science and has gained award for her exemplary academic performance. She has been engaged in creating and delivering content in the cloud and linux space. She can be reached out LinkedIn or via email. ","date":"2022-07-11","objectID":"/ansible_sd/:3:0","tags":["How-to","technology","aws","windows","Red Hat","RHEL","ansible","ShreyaDhange","VisitingAuthors"],"title":"Managing Windows Server from Red Hat Server using Ansible on AWS EC2","uri":"/ansible_sd/"},{"categories":["VisitingAuthors","How-to","technology"],"content":" Ansible is an open-source configuration management and deployment tool. Below are the steps to run docker container on AWS EC2 instances using Ansible playbook. Create AWS EC2 instances (master and slave) Update the security group of slave exposing port 8080 Install Ansible on master Install python on slave Update /etc/ansible/hosts on master with the slave IP Create a dockerfile which will be used to create customized image Create an ansible playbook with detailed tasks Execute the ansible playbook Check if the docker container is running successfully on the slave machine ","date":"2022-07-09","objectID":"/docker_containters_ansibleaws/:0:0","tags":["How-to","docker","blog","ansible","aws","technology","VrindaHegde","SiyaAmonkar","VisitingAuthors"],"title":"Docker containers using Ansible on AWS","uri":"/docker_containters_ansibleaws/"},{"categories":["VisitingAuthors","How-to","technology"],"content":"Install Ansible on master machine: Execute the below commands to install Ansible on the master machine: sudo apt-get update sudo apt install software-properties-common sudo apt-add-repository ppa:ansible/ansible sudo apt-get install ansible ","date":"2022-07-09","objectID":"/docker_containters_ansibleaws/:1:0","tags":["How-to","docker","blog","ansible","aws","technology","VrindaHegde","SiyaAmonkar","VisitingAuthors"],"title":"Docker containers using Ansible on AWS","uri":"/docker_containters_ansibleaws/"},{"categories":["VisitingAuthors","How-to","technology"],"content":"Install Python on slave machine: Use the below commands to install python on the slave machine: sudo apt-get update sudo apt-get install python On the master machine perform the below steps to create ssh-keygen: cd /home/ubuntu/.ssh ssh-keygen File names id_rsa.pub will be generated after executing ssh-keygen on master machine. on master machine Copy the contents of the id_rsa.pub file and paste it on slave machine inside the authorized_keys file located at /home/ubuntu/.ssh on slave machine ","date":"2022-07-09","objectID":"/docker_containters_ansibleaws/:2:0","tags":["How-to","docker","blog","ansible","aws","technology","VrindaHegde","SiyaAmonkar","VisitingAuthors"],"title":"Docker containers using Ansible on AWS","uri":"/docker_containters_ansibleaws/"},{"categories":["VisitingAuthors","How-to","technology"],"content":"Update the hosts file on master: To establish ssh connection with the slave machine, add the IP of the slave machine on master’s hosts file. To edit the hosts file use the below command: sudo nano /etc/ansible/hosts Paste the below content at the end of the file: [production] slave ansible_ssh_host=\u003cslave-IP\u003e To verify the ssh connection between the master and slave, execute the below command: output of the ping command on master machine ","date":"2022-07-09","objectID":"/docker_containters_ansibleaws/:3:0","tags":["How-to","docker","blog","ansible","aws","technology","VrindaHegde","SiyaAmonkar","VisitingAuthors"],"title":"Docker containers using Ansible on AWS","uri":"/docker_containters_ansibleaws/"},{"categories":["VisitingAuthors","How-to","technology"],"content":"Creating the code directory on master machine: Create a directory named LAMP_STACK_content at /home/ubuntu/ location on the master machine. Create a dockerfile at this location. ","date":"2022-07-09","objectID":"/docker_containters_ansibleaws/:4:0","tags":["How-to","docker","blog","ansible","aws","technology","VrindaHegde","SiyaAmonkar","VisitingAuthors"],"title":"Docker containers using Ansible on AWS","uri":"/docker_containters_ansibleaws/"},{"categories":["VisitingAuthors","How-to","technology"],"content":"Create a dockerfile: In order to create a customized image we will create a dockerfile with the following contents: # Dockerfile for LAMP Stack installation # Ubuntu 18.04 image FROM ubuntu:18.04 ENV DEBIAN_FRONTEND=noninteractive RUN apt-get update -y RUN apt-get upgrade -y # Install apache RUN apt-get install -y apache2 # Prerequisites for installing php7.3 RUN apt-get install -y software-properties-common RUN add-apt-repository ppa:ondrej/php RUN apt install -y php7.3-fpm # Install php7.3 for this set up RUN apt install -y php7.3 # Extensions of php RUN apt install php7.3-common php7.3-mysql php7.3-xml php7.3-xmlrpc php7.3-curl php7.3-gd php7.3-imagick php7.3-cli php7.3-dev php7.3-imap php7.3-mbstring php7.3-opcache php7.3-soap php7.3-zip php7.3-intl -y # Removing the default index.html page and copying the project code RUN rm -f /var/www/html/index.html COPY . /var/www/html/ # Install ufw RUN apt install ufw -y RUN ufw app list # install library RUN apt-get install libapache2-mod-php7.3 # install additional packages RUN a2dismod mpm_event \u0026\u0026 a2enmod mpm_prefork \u0026\u0026 a2enmod php7.3 # Restart apache RUN service apache2 restart # Provide executable permissions to the code RUN chmod -R 0777 /var/www/html/* RUN chmod -R 0777 /var/* # Change WORKDIR WORKDIR /var/www/html CMD [\"apachectl\",\"-D\",\"FOREGROUND\"] RUN a2enmod rewrite EXPOSE 80 EXPOSE 443 ","date":"2022-07-09","objectID":"/docker_containters_ansibleaws/:5:0","tags":["How-to","docker","blog","ansible","aws","technology","VrindaHegde","SiyaAmonkar","VisitingAuthors"],"title":"Docker containers using Ansible on AWS","uri":"/docker_containters_ansibleaws/"},{"categories":["VisitingAuthors","How-to","technology"],"content":"Create ansible playbook: Create an ansible-playbook-lamp-stack-new.yaml file on master with the below content. --- - hosts: slave become: yes gather_facts: no tasks: - name: create build directory file: path: /root/demo-lamp_stack state: directory owner: root group: root mode: '0755' - name: copy Dockerfile copy: src: /home/ubuntu/LAMP_STACK_content dest: /root/demo-lamp_stack/ owner: root group: root mode: '0644' - name: build and push container image community.docker.docker_image: build: path: /root/demo-lamp_stack/LAMP_STACK_content/ name: dock1998/lamp_stack_new:v1 source: build state: present - name: Running the container community.docker.docker_container: image: dock1998/lamp_stack_new:v1 name: lamp_stack_new_cont state: started ports: \"8080:80\" - name: Tag and push to docker hub community.docker.docker_image: name: dock1998/lamp_stack_new:v1 repository: dock1998/lamp_stack_new:v2 push: yes source: local state: present ","date":"2022-07-09","objectID":"/docker_containters_ansibleaws/:6:0","tags":["How-to","docker","blog","ansible","aws","technology","VrindaHegde","SiyaAmonkar","VisitingAuthors"],"title":"Docker containers using Ansible on AWS","uri":"/docker_containters_ansibleaws/"},{"categories":["VisitingAuthors","How-to","technology"],"content":"Execute the ansible playbook: Note: Before proceeding ahead with the next steps, you will need to perform docker login using the dockerhub account on the slave machine from where the image will be pushed and pulled. To execute the playbook use the below command: ansible-playbook ansible-playbook-lmap-stack-new.yaml output of the above command on master machine ","date":"2022-07-09","objectID":"/docker_containters_ansibleaws/:7:0","tags":["How-to","docker","blog","ansible","aws","technology","VrindaHegde","SiyaAmonkar","VisitingAuthors"],"title":"Docker containers using Ansible on AWS","uri":"/docker_containters_ansibleaws/"},{"categories":["VisitingAuthors","How-to","technology"],"content":"To verify the execution of playbook: Check if the code directory is copied on the slave machine. output on slave Check if the docker container is running on the slave machine: docker ps output of docker ps on the slave machine Note: If you get permission denied error after executing docker ps command then you need to give permissions to the docker.sock file by executing the below command: chmod -R 0777 /var/run/docker.sock On the slave machine add the in-bound rules to direct traffic to the port 8080 by updating the security groups on the AWS console as shown below. AWS console for slave machine Access the slave IP with the exposed port 8080 on the browser and the application should be up. Application running on port 8080 Hope the article was useful, thanks for reading! Happy learning. 👍 Vrinda Hegde is a DevOps Engineer, who likes to explore orchestration tools and automate the process of deploying containerized applications. She likes to share her findings by writing articles on medium.com. She can be reached out on LinkedIn or via email Siya Amonkar is a DevOps Engineer who likes to explore new tools and technologies. She can be reached out on LinkedIn or via email ","date":"2022-07-09","objectID":"/docker_containters_ansibleaws/:8:0","tags":["How-to","docker","blog","ansible","aws","technology","VrindaHegde","SiyaAmonkar","VisitingAuthors"],"title":"Docker containers using Ansible on AWS","uri":"/docker_containters_ansibleaws/"},{"categories":["community","UnconventionalContributors"],"content":"We have many diverse contributors in opensource that help upstream communities in unconventional ways, which doesn’t require any coding or development skills. Starting with a non-code contribution can help anyone overcome the sense of failure and not being good enough, and it can also serve as a springboard for our open source adventure. This interview series aims to highlight some non-code open source contributions that anyone can make right now to get started contributing. For this month’s edition, we talked about the Mozilla and Wikipedia with Prathamesh Chavan. Can you tell us some background about you, what your journey has been at Red Hat, and what you do today? I am Prathamesh Chavan and I completed my engineering degree in information technology in 2016 and joined Red Hat as an intern on October 3rd, 2016. I was converted to full-time employment in 2017 and joined the Technical Chat Support team as an Associate Customer Support Specialist-Technical. As a Technical Support Engineer, I worked on queries related to subscription management, and it was in March of 2020 when I joined the then CEE Operations team as a Technical Project Coordinator. As of today, I am an Associate Technical Project Manager in the CEE Strategic Solutions team, and my day- to-day responsibilities involve managing agile projects. Share your experience in becoming a community contributor. An open source contribution is a selfless effort to do something good. After becoming an open source contributor, I was able to explore the various steps of the software development life cycle. I learned how it provides a basis for project planning, scheduling, and estimating, as well as how it raises project planning visibility among all the stakeholders. I felt very empowered and motivated due to the open source ideology, and this has helped me in building a “never stop, never-give-up” attitude. What project(s) have you contributed to? How did your contribution journey start? I have contributed to Mozilla projects and Wikipedia. I joined the technical club at my university as an events blogger, and it was only after that that I came to know about the various open source communities and the contribution pathways. In my initial days, I read a couple of articles in the Mozilla Developers Network portal related to the Mozilla Firefox browser and started to suggest edits wherever needed. Later, I started localizing the Mozilla articles, which were written in English, into my native language, Marathi. Over the next couple of months, I got acquainted with the Mozilla Firefox browser and started helping other users fix their issues with the browser. I also believe most open source organizations consider answering other people’s questions on Quora or Reddit to be a useful contribution. I started joining and reading discussions on threads and learned a lot through that. So, I always recommend to people that if we notice a question and know the solution, we may try to help the person who asked it by answering it, and our responses will be counted as contributions to the project. Sometimes, assuming one doesn’t know the perfect solution to the problem, simply helping others comprehend why the problem arises may be enough to allow them to come up with their own solution. You may help manage the discussion threads or community chat channels by answering questions about problems on GitHub, opensource.com, Mozilla, etc. In what way(s) have you contributed? As someone who likes talking about open source philosophies and concepts, I decided to start with a documentation-related contribution as a technical writer, where I started writing and reviewing the documentation on the Mozilla Developers Network. Later, I realized that the articles were only written in English, and there was a huge need to localize such articles in the regional languages. I started my contribution by localizing the articles and strings. Due to such contributions, I got some knowledge about how to use the Fi","date":"2022-07-05","objectID":"/contributorblog_1/:0:0","tags":["blogs","community","fedora","mozilla"],"title":"Contributor Blog #1: Pilot","uri":"/contributorblog_1/"},{"categories":null,"content":"Hey there! TL;DR: Name: Parth Goswami Let’s connect on Twitter ","date":"2022-06-26","objectID":"/about/:0:0","tags":null,"title":"About","uri":"/about/"},{"categories":null,"content":"About the website A platform for the community, by the community VistingAuthors Many of my colleagues are avid writers who frequently document their knowledge and experiences. Some of them use their own platforms to publish the information, while others use blogs like Medium and Blogger to publish it. The VisitingAuthors area of this website is an effort to offer one such platform for authors to share their knowledge and ideas. Please check out the site; I hope it will provide you with useful content. UnconventionalContributors We have a diverse range of contributors in the open source community who make contributions in unconventional ways without using any coding or developing expertise, some ways we hear quite often, some not so much. This high-level instructional interview series should be beneficial to anyone who wants to start contributing to open source. One such forum is the UnconventionalContributors series, which is designed for aspiring and emerging contributors to get involved with open source communities. ","date":"2022-06-26","objectID":"/about/:1:0","tags":null,"title":"About","uri":"/about/"},{"categories":null,"content":"About the author I am a Customer Enablement Engineer working for Cloudera on the Professional Services Team in Bengaluru, India. My interests are in Kubernetes, cloud, and cloud native related projects. I like to contribute to OpenSource in any possible way. The opinions stated here are my own, not necessarily those of my company. ","date":"2022-06-26","objectID":"/about/:2:0","tags":null,"title":"About","uri":"/about/"},{"categories":null,"content":"About copyright All original articles on this site are protected by the Creative Commons Attribution-NonCommercial 4.0 License/CC BY-NC 4.0 . Copyright statement You are free to: Share — copy and redistribute the material in any medium or format Adapt — remix, transform, and build upon the material The licensor cannot revoke these freedoms as long as you follow the license terms. Any individual or media should abide by the following copyright requirements when reproducing the original content of this site (including text, self-made images, and photographic works): indicate reprint indicate the source as the site domain name ( parthgoswami.com ), or the full URL where the reprinted content is located NonCommercial — You may not use the material for commercial purposes. Except for original works, most of the pictures on this site come from the Internet. The original copyright owner of such pictures may request this site to stop using relevant pictures at any time and for any reason, including pictures edited by this site (such as annotated descriptions). ","date":"2022-06-26","objectID":"/about/:3:0","tags":null,"title":"About","uri":"/about/"},{"categories":null,"content":"Come, say hello!👋🏻 Get in touch with us, we would love to talk to you. ","date":"2022-06-26","objectID":"/about/:4:0","tags":null,"title":"About","uri":"/about/"},{"categories":["How-to","VisitingAuthors","technology"],"content":"Consider a scenerio: We have setup a VPC for the instances which are in the same range. The Public instance is having an internet connection and Private instance does not having an internet connection. Inside Public instance we have S3 bucket endpoint to which users will have the access to upload/download images and videos. The EC2 instances have an ELB (Elastic Load Balancer) attached with an Alarm set to it. The EC2 instances will have IAM roles attached to it having roles set like AWSS3FullAccess, AWSRekognitionFullAccess and AWSElasticTranscoderFullAccess. ","date":"2022-06-26","objectID":"/aws_rekognition/:1:0","tags":["How-to","technology","aws","Amazon S3","aws rekognition","aws transcoder","ShreyaDhange","VisitingAuthors"],"title":"How to configure AWS Rekognition and AWS Transcoder","uri":"/aws_rekognition/"},{"categories":["How-to","VisitingAuthors","technology"],"content":"Steps: Private instance will have NAT connectivity with Public instance which will open a browser with the endpoint given to the user to access S3 bucket uploads an image an request for image recognition. Inside EC2 run this command to get the output: aws rekognition detect-labels --image \"{\\\"S3Object\\\":{\\\"Bucket\\\":\\\"bucketname\\\",\\\"Name\\\":\\\"image.png\\\"}}\" --region us-east-1 The output of the image recognition will be stored in Amazon RDS. If the user wants a video to be converted to the version which can be accessible via phone or pc can upload the video in S3 Bucket and by setting up a pipeline in AWS Transcoder the user will be notified about the process completion and the user can further request for image/video recognition. The user having internet connection will access S3 Bucket with the endpoint provided and can further request for the AWS Rekognition. Procedure is same as above. ","date":"2022-06-26","objectID":"/aws_rekognition/:2:0","tags":["How-to","technology","aws","Amazon S3","aws rekognition","aws transcoder","ShreyaDhange","VisitingAuthors"],"title":"How to configure AWS Rekognition and AWS Transcoder","uri":"/aws_rekognition/"},{"categories":["How-to","VisitingAuthors","technology"],"content":"Configuration: First step is to configure VPC: 1.1 Create VPC with the name MyVPC: 1.2 Create 3 Subnets with the name Public, Public1, Private: 1.3 Create a separate Route table with names PublicRT and PrivateRT and place the subnets with the respective route table: 1.4 Create a Gateway with the name MyGateway: Create three EC2 instances and attach them with MyVPC as given below : Create IAM roles the instances: 3.1 Create IAM roles for Public Instance with respective policy as shown below: 3.2 Create IAM role for Private instance which holds RDS policy (optional) : Create S3 bucket as follows: 4.1 Create 2 buckets with the name inputbucket12 and outputbucket12: 4.2 The inputbucket12 will be used to upload images and videos: Setup for AWS Image Rekognition in Public instance using cli mode: Setting up AWS Transcoder for video converter: 6.1 Create Pipeline as given below: 6.2 Create job for the Pipeline: 6.3 Output of the video will be shown in outputbucket12: Shreya Dhange is a Technical Training Developer at Red Hat, who likes to explore and learn new technologies and share her knowledge by writing articles. She has completed her Masters in Computer Science and has gained award for her exemplary academic performance. She has been engaged in creating and delivering content in the cloud and linux space. She can be reached out LinkedIn or via email. ","date":"2022-06-26","objectID":"/aws_rekognition/:3:0","tags":["How-to","technology","aws","Amazon S3","aws rekognition","aws transcoder","ShreyaDhange","VisitingAuthors"],"title":"How to configure AWS Rekognition and AWS Transcoder","uri":"/aws_rekognition/"},{"categories":["VisitingAuthors","How-to","technology"],"content":"Let us create a dockerfile for deploying the application. In this dockerfile we will use ubuntu image as the base image and then install apache and php above it. Then we will create a MYSQL container which will be connected to our application. ","date":"2022-06-06","objectID":"/dockerizing_lamp_stack_app/:0:0","tags":["How-to","docker","blog","lamp stack","technology","VrindaHegde","VisitingAuthors"],"title":"Dockerizing LAMP Stack Application","uri":"/dockerizing_lamp_stack_app/"},{"categories":["VisitingAuthors","How-to","technology"],"content":"Below is the Dockerfile snippet: # Dockerfile for LAMP Stack installation # Ubuntu 18.04 image FROM ubuntu:18.04 ENV DEBIAN_FRONTEND=noninteractive RUN apt-get update -y RUN apt-get upgrade -y # Install apache RUN apt-get install -y apache2 # Prerequisites for installing php7.3 RUN apt-get install -y software-properties-common RUN add-apt-repository ppa:ondrej/php RUN apt install -y php7.3-fpm # Install php7.3 for this set up RUN apt install -y php7.3 # Extensions of php RUN apt install php7.3-common php7.3-mysql php7.3-xml php7.3-xmlrpc php7.3-curl php7.3-gd php7.3-imagick php7.3-cli php7.3-dev php7.3-imap php7.3-mbstring php7.3-opcache php7.3-soap php7.3-zip php7.3-intl -y # Removing the default index.html page and copying the project code RUN rm -f /var/www/html/index.html COPY . /var/www/html/ # Install ufw RUN apt install ufw -y RUN ufw app list # install library RUN apt-get install libapache2-mod-php7.3 # install additional packages RUN a2dismod mpm_event \u0026\u0026 a2enmod mpm_prefork \u0026\u0026 a2enmod php7.3 # Restart apache RUN service apache2 restart # Provide executable permissions to the code RUN chmod -R 0777 /var/www/html/* RUN chmod -R 0777 /var/* # Change WORKDIR WORKDIR /var/www/html CMD [\"apachectl\",\"-D\",\"FOREGROUND\"] RUN a2enmod rewrite EXPOSE 80 EXPOSE 443 In this dockerfile I have installed php 7.3 version which was required for my application. Use the below command to build the image from the dockerfile: docker build -f dockerfile-lamp-stack.dockerfile . Next let us create the containers to deploy the application. Below is the snippet of docker-compose file: version: '3' networks: lamp-stack-net: external: true volumes: mysql_storage_01: external: true services: lamp_stack: image: lamp_stack_app:v1 privileged: true build: context: path_to_code dockerfile: dockerfile-lamp-stack.dockerfile container_name: app_cont networks: - lamp-stack-net ports: - \"8010:80\" volumes: - path_to_code/:/var/www/html/ mysql_service: image: mysql:5.7.25 container_name: mysql_cont ports: - \"3306:3306\" environment: # MYSQL_ROOT_PASSWORD: '' # MYSQL_ALLOW_EMPTY_PASSWORD : 'yes' MYSQL_ROOT_PASSWORD: root MYSQL_DATABASE: test_db MYSQL_USER: test_user MYSQL_PASSWORD: test@123 networks: - lamp-stack-net restart: always volumes: - path_to_dump_file/:/home/ - mysql_storage_01:/var/lib/mysql In the above docker-compose file we are building the LAMP Stack container and MYSQL container. For the lamp_stack service we need to give the context of the code and place the dockerfile at that loaction in order to build our LAMP Stack image. We are exposing the port 8010 where the application will be served on the browser. Next is mysql_service, where we are creating the mysql container by using MYSQL 5.7 version. In the volumes section we have to use the location to our source code and database dump file respectively on line 25 and 43 respectively. Use the below commands to create network and volumes respectively: docker network create lamp-stack-net docker volume create --name=mysql_storage_01 Now let us create the containers using the below command: docker-compose -f docker-compose-lamp-stack.yml up -d Now check if both the containers are up and running using the below command: docker ps If both the containers are up and running then check on the browser using: \u003cIP\u003e:8010 or localhost:8010 This will serve the default page on the browser. To connect to the database you will need to use the database details in your php config file. Hope this article was helpful. Happy Learing!!! Vrinda Hegde is a DevOps Engineer, who likes to explore orchestration tools and automate the process of deploying containerized applications. She likes to share her findings by writing articles on medium.com. She can be reached out on LinkedIn or via email ","date":"2022-06-06","objectID":"/dockerizing_lamp_stack_app/:1:0","tags":["How-to","docker","blog","lamp stack","technology","VrindaHegde","VisitingAuthors"],"title":"Dockerizing LAMP Stack Application","uri":"/dockerizing_lamp_stack_app/"},{"categories":["How-to","technology"],"content":"Kubernetes won the race to become the de-facto container orchestration engine. Ever since, a lot of companies are coming up with their own version of container orchestration based on top of Kubernetes, while, some other companies are developing tools and products to complete the Kubernetes in the orchestration space. Having said that, the top three cloud provides, have come up with their own version of managed Kubernetes offerings: Microsoft Azure offers the Azure Kubernetes Service (AKS) AWS offers the Amazon Elastic Kubernetes Service (EKS) Google Cloud offers the Google Kubernetes Engine (GKE) ","date":"2022-06-03","objectID":"/amazoneks/:0:0","tags":["How-to","blog","aws","AmazonEKS","technology"],"title":"Amazon EKS: A managed Kubernetes service by AWS","uri":"/amazoneks/"},{"categories":["How-to","technology"],"content":"What is Amazon EKS? Amazon Elastic Kubernetes Service (Amazon EKS) is a managed service that you can use to run Kubernetes on AWS without needing to install, operate, and maintain your own Kubernetes control plane or nodes. Kubernetes is an open-source system for automating the deployment, scaling, and management of containerized applications. In June 2018, AWS made EKS generally available for all. EKS is similar to AKS and GKE as it supports swift development and deployment of applications based on Kubernetes. Let’s take a look at some of the critical areas to understand how EKS behaves as a managed Kubernetes service. ","date":"2022-06-03","objectID":"/amazoneks/:1:0","tags":["How-to","blog","aws","AmazonEKS","technology"],"title":"Amazon EKS: A managed Kubernetes service by AWS","uri":"/amazoneks/"},{"categories":["How-to","technology"],"content":"Updates Updating EKS requires multiple steps to be implemented. The users of EKS need to run instructions via command-line, which initiates the update. These steps are required to manage the updates for nodes. ","date":"2022-06-03","objectID":"/amazoneks/:2:0","tags":["How-to","blog","aws","AmazonEKS","technology"],"title":"Amazon EKS: A managed Kubernetes service by AWS","uri":"/amazoneks/"},{"categories":["How-to","technology"],"content":"Resource monitoring AWS offers lightweight monitoring for the control plane directly in Cloudwatch. To monitor the workers, you can use Kubernetes Container Insights Metrics provided via a specific CloudWatch agent you can install in the cluster. ","date":"2022-06-03","objectID":"/amazoneks/:3:0","tags":["How-to","blog","aws","AmazonEKS","technology"],"title":"Amazon EKS: A managed Kubernetes service by AWS","uri":"/amazoneks/"},{"categories":["How-to","technology"],"content":"Availability AWS has 66 availability zones. And the footprint will increase as Amazon plans to add 12 more zones to its total tally of AZs. ","date":"2022-06-03","objectID":"/amazoneks/:4:0","tags":["How-to","blog","aws","AmazonEKS","technology"],"title":"Amazon EKS: A managed Kubernetes service by AWS","uri":"/amazoneks/"},{"categories":["How-to","technology"],"content":"CLI Support The official user guide also uses the following command line tools: kubectl – A command line tool for working with Kubernetes clusters. eksctl – A command line tool for working with EKS clusters that automates many individual tasks. AWS CLI – A command line tool for working with AWS services, including Amazon EKS. Creating a cluster via eksctl is as easy as eksctl create cluster, no other parameters required. ","date":"2022-06-03","objectID":"/amazoneks/:5:0","tags":["How-to","blog","aws","AmazonEKS","technology"],"title":"Amazon EKS: A managed Kubernetes service by AWS","uri":"/amazoneks/"},{"categories":["How-to","technology"],"content":"Node Pools In a cluster of K8s, a group of nodes that share the same configuration are referred to as a node pool. Node pools are vital as they allow the cluster to function with different machines for various workloads. The users can designate the node pools with the service they want to deploy them with. EKS allows its users to run 100 nodes per node pool. ","date":"2022-06-03","objectID":"/amazoneks/:6:0","tags":["How-to","blog","aws","AmazonEKS","technology"],"title":"Amazon EKS: A managed Kubernetes service by AWS","uri":"/amazoneks/"},{"categories":["How-to","technology"],"content":"Auto-Scaling One of the standout features of Kubernetes is its seamless ability to scale the nodes. This enables the cluster to trim down on resources usage. This not only saves time but is also cost-effective as both heavy and lean demands are met with the right amount of resources. Additionally, Auto-scaling can be utilized to tweak the resource utilization plans for the present and future. EKS autoscaling is relatively easy as it only takes a few manual steps. Also, it is the only one to allow bare-metal nodes to run your Kubernetes cluster. ","date":"2022-06-03","objectID":"/amazoneks/:7:0","tags":["How-to","blog","aws","AmazonEKS","technology"],"title":"Amazon EKS: A managed Kubernetes service by AWS","uri":"/amazoneks/"},{"categories":["How-to","technology"],"content":"Pricing EKS bills 10 cents/hour/control plane. Additionally, USD 0.20 per hour is billed for every deployed cluster. AWS doesn’t allow the use of their free tier to test an EKS cluster is that EKS requires bigger machines than the tX.micro tier, and EKS hourly pricing is not in the free tier. Hope the article was useful, thanks for reading! ","date":"2022-06-03","objectID":"/amazoneks/:8:0","tags":["How-to","blog","aws","AmazonEKS","technology"],"title":"Amazon EKS: A managed Kubernetes service by AWS","uri":"/amazoneks/"},{"categories":["How-to","technology"],"content":" Nope! This is article is not about what AWS is, how it works and what are its services, rather it talks about AWS and how to get started with it being a complete beginner. I am working on AWS for more than 5+ years now and have taught professionally for 2 years, hence, I understand how overwhelming it might get for a beginner to figure out how to start studying AWS. For many, due to one or the other reasons, AWS is the first public cloud provider which they get exposed to. Hence, this article aims at providing you a detailed overview about AWS and its ecosystem and how to get started with it. I plan to write a similar blogpost on the other two major cloud providers, Google Cloud Platform [GCP] and Microsoft Azure, but that’s for another day once I myself get a good hold on them. This article is for a student, who is just starting with their cloud journey, or for a software developer, tester, technical sales who are new to AWS or cloud for that matter. If your team is looking to invest in AWS, or your company is undergoing digital transformation, or if you are a startup looking to migrate some/entire load on to AWS, then this article is for you. I hope this article serves you’ll well. Let’s get started! ","date":"2022-06-03","objectID":"/aws_for_beginners/:0:0","tags":["How-to","blog","aws","technology"],"title":"How to get started on AWS","uri":"/aws_for_beginners/"},{"categories":["How-to","technology"],"content":"What is AWS? AWS stands for Amazon Web Services. It offers variety of models, the most famous being pay-as-you-go, to work with the basic infrastructure you would need to run your company. The services include range of products in the compute, storage, networking, database, analytics and many other domain. It is a comprehensive cloud computing platform that includes infrastructure as a service (IaaS), platform as a service (PaaS) as well as sofware as a service (SaaS) offerings. services ","date":"2022-06-03","objectID":"/aws_for_beginners/:1:0","tags":["How-to","blog","aws","technology"],"title":"How to get started on AWS","uri":"/aws_for_beginners/"},{"categories":["How-to","technology"],"content":"Start with core services AWS offers more than 150+ services, which can be quite intimidating for a beginner. However, you don’t need to master them all. You can start with the core services which acts as a building blocks of any cloud provider and also help you get certified eventually. I always recommend to start with Identity and Access Management (IAM) service for couple of reasons. Since it is absolutely free of cost: You get a good grasp of AWS console while exploring and learning IAM Even if you mess up with IAM, your account most probably will not get charged Below are some of the key “building block” services which form the core of the AWS platform. Getting familiar with these is a good place to start your learning: Elastic Compute Cloud (EC2): virtual servers Relational Database Service (RDS): relational databases Elastic Block Store (EBS): block storage Simple Storage Service (S3): file storage Identity and Access Management (IAM): users, groups and roles Virtual Private Cloud (VPC): networking Remember Learn core AWS services first Prioritise hands-on learning Structure your learning ","date":"2022-06-03","objectID":"/aws_for_beginners/:2:0","tags":["How-to","blog","aws","technology"],"title":"How to get started on AWS","uri":"/aws_for_beginners/"},{"categories":["How-to","technology"],"content":"Certifications Getting certified on AWS have great benefits and you don’t have to get certified on all of them. Depending on your role/domain/aspirations, you select a specific track and prepare for accordingly. The good thing is, AWS certificaion exams are simplified and borken down into different categories. Associate: The associate exams are your more entry level exams, Professional: The professional exams build on top of the associate exams with more detail. Specialty: You can also go down a specialty route and learning a specific topic like Networking or Security. AWS Certifications The best place to start as a complete beginner is with the Cloud Practitioner exam. The Cloud Practitioner exam is going to give you a solid basis in AWS. When you’ve completed the Cloud Practitioner exam, you can then take a look at one of the associate exams, Architect, SysOps or Developer depending on your preference. ","date":"2022-06-03","objectID":"/aws_for_beginners/:3:0","tags":["How-to","blog","aws","technology"],"title":"How to get started on AWS","uri":"/aws_for_beginners/"},{"categories":["How-to","technology"],"content":"Resources Alright, since we have covered what AWS is, what are its services and what certifications AWS provides, I think it’s time to talk about the resources to master those services. Documentation youtube stackoverflow aws.amazon.com/free In addition to these resources, also checkout: • Whitepapers: Resources designed to broaden your technical understanding, written by the AWS team, independent analysts, and AWS partners. • FAQ: Commonly raised issues and questions that will help you understand AWS products, services, and features beyond the scope of your personal experience. ","date":"2022-06-03","objectID":"/aws_for_beginners/:4:0","tags":["How-to","blog","aws","technology"],"title":"How to get started on AWS","uri":"/aws_for_beginners/"},{"categories":["How-to","technology"],"content":"Prerequisites Before you start with hands-on practice, you need: AWS free tier account to kick off journey Credit card or debit card is mandatory ( It will not charge if you are using the free tier resources properly. Do checkout the free tier limit for the services you are working with). ","date":"2022-06-03","objectID":"/aws_for_beginners/:5:0","tags":["How-to","blog","aws","technology"],"title":"How to get started on AWS","uri":"/aws_for_beginners/"},{"categories":["How-to","technology"],"content":"Events AWS hosts events, both online and in-person, bringing the cloud computing community together to connect, collaborate, and learn from AWS experts. These events ranges from AWS Summits, to partner events, webinars, training and certification and more. Check out the events page for more details. ","date":"2022-06-03","objectID":"/aws_for_beginners/:6:0","tags":["How-to","blog","aws","technology"],"title":"How to get started on AWS","uri":"/aws_for_beginners/"},{"categories":["How-to","technology"],"content":"Careers AWS offers exciting and variety of roles around the globe. Check out the careers page for more details. Note: AWS frequently keeps adding new services to its current pool and reguarly comes up with new certifications. Also, the links that I have shared in this articles may get updated. If you come across any such instance, feel free to reach out to me so that it will help me keep this article updated. Happy learning! ","date":"2022-06-03","objectID":"/aws_for_beginners/:7:0","tags":["How-to","blog","aws","technology"],"title":"How to get started on AWS","uri":"/aws_for_beginners/"},{"categories":["VisitingAuthors","How-to","technology"],"content":" AWS Lambda is a compute service that lets you run code without provisioning or managing servers. Lambda runs your code only when needed and scales automatically, from a few requests per day to thousands per second. AWS Lambda in our task is invoked using the API Gateway. Slack is a channel-based messaging platform. With Slack, people can work together more effectively, connect all their software tools and services, and find the information they need to do their best work — all within a secure, enterprise-grade environment. Invoke lambda function through API Gateway from Slack This blog covers on how to invoke AWS Lambda function through API Gateway when there is a new message posted to any Slack channel. Here are a few business cases where this can be used: If a Quality Analyst wants to trigger an integration test by sending a message via Slack. If a Developer wants to get logs of a Load Balancer which is managed by the Security Team. Here are the steps to be followed…. Step1: First let’s configure Slack Create a workspace in slack.com, then create a slack app at api.slack.com/apps in the newly created workspace and then go to OAuth \u0026 Permissions tab, add channels:history, channels:read, chat:write, im:history \u0026 mpim:history OAuth scope to Bot Token Scopes and User Token Scopes. Now install your app to workspace and allow necessary permissions, then User OAuth Token \u0026 Bot User OAuth Token are generated. Bot and User Token Scopes OAuth \u0026 Permissions page And also keep the copy of verification token which is present in the Basic Information tab of your app. Access Verification Token from Basic Information page Step 2: Now let’s move on to AWS Lambda configuration part Let’s create a Lambda function using Author from scratch lambda function template. Select runtime as Node.js and Create a new role with basic Lambda permissions. Creating Lambda function in AWS Lambda Console Now your lambda function is created. In the code tab, change index.js file as follows. //Verify Url - https://api.slack.com/events/url_verification function verify(data, callback) { if (data.token === VERIFICATION_TOKEN) callback(null, data.challenge); else callback(\"verification failed\"); } //Print the slack message on the console function process(data,context, callback) { console.log('context:', JSON.stringify(context)); console.log('data.event.text',JSON.stringify(data.event.text)); callback(null,data.event.message); } // Lambda handler exports.handler = (data, context, callback) =\u003e { console.log(data); switch (data.type) { case \"url_verification\": verify(data, callback); break; case \"event_callback\": process(data,context, callback);break; default: callback(null,\"Hello from Lambda\"); } }; [APP VERIFICATION TOKEN]: Access this token in the basic information tab of the slack app created in Step 1(Refer to Access Verification Token from Basic Information page figure in Step 1). [SLACK ACCESS TOKEN]: Access Bot User OAuth Token from OAuth Tokens for Your Team in OAuth \u0026 Permissions page(Refer to OAuth \u0026 Permissions page figure in Step 1). Step 3: In this step, API Gateway is configured Add a REST API gateway trigger to the lambda function and select Security as Open. In the additional settings, change the deployment stage name to prod(Optional). After the trigger is added, go to API gateway console, add method POST to the resource. After creating POST method, then select Deploy API option from the Actions menu. Add POST method to the SlackLambda resource Select Deploy API Select the Deployment stage which was created in the earlier step, then click deploy Deploy API to prod stage Step 4: Slack Configuration post AWS Lambda/API Gateway configuration From Slack App(api.slack.com/apps), select the newly created app. Then go to Event Subscriptions and Enable Events. In the Request URL textbox, paste the API gateway URL which can be accessed from the configuration tab of Lambda function. Enabling EVents in Event Subscription page Proceed once the URL is verified. S","date":"2022-06-03","objectID":"/lambda_slack/:0:0","tags":["How-to","aws","blog","aws lambda","slack","technology","SiyaAmonkar","VarshareddyKumbham","VisitingAuthors"],"title":"Invoke AWS Lambda from Slack","uri":"/lambda_slack/"},{"categories":["VisitingAuthors","How-to","technology"],"content":"Now let’s test it! Open slack.com and Launch the workspace where the app is created then post the message to a slack channel or a direct message. Message posted to slack channel Now check the message and metadata of the message in the cloud-watch log console. To conclude, When someone posts a message to a slack channel, the API gateway triggers the lambda function and cloud watch logs has a record of all the Lambda invocations. We have come to an end of this blog. Hope you liked it, please do share it with your friends. Happy Reading! Siya Amonkar is a DevOps Engineer who likes to explore new tools and technologies. She can be reached out on LinkedIn or via email Varsha is a software engineer with overall experience of 2 years, who loves to play with devops tools and is a cloud computing enthusiast. She can be reached out on LinkedIn or via email ","date":"2022-06-03","objectID":"/lambda_slack/:1:0","tags":["How-to","aws","blog","aws lambda","slack","technology","SiyaAmonkar","VarshareddyKumbham","VisitingAuthors"],"title":"Invoke AWS Lambda from Slack","uri":"/lambda_slack/"},{"categories":["How-to","VisitingAuthors","technology"],"content":"Amazon S3 is used to store any amount of data of object type such as static websites, documents, images, videos and backup files as well. This data can be retrieved at any time from anywhere on the web. Inside S3 you create Buckets to store your objects/data. You can create multiple buckets in any region and it stores data upto 5TB. After you create buckets and upload objects in Amazon S3, you can manage your object storage using below features: Versioning: You can create multiple copies of your data. If the data center goes down in any region your data will be maintained in other data center of that region. If your data gets deleted versioning helps in retrieving it as it creates version ID of your data. It prevents overwriting or accidential deletion of data. As and when you make any changes in your data the latest copy will be available on the top. Cross region replication: If you want to copy data from one region bucket to another region bucket you can do it using cross region replication. Transfer acceleration: Transferring the data from one location to another location with low latency. Transfer acceleration puts data to CloudFront Edge Location and transfers your data. Amazon CloudFront is been explained in another blog. Lifecyle: You can decide the lifecyle of an object by just settings its lifecycle. For example: Now if your objects are in Standard-IA and after few days you want those objects to be in Glacier in that case you can set lifecycle to your objects. There are classes of S3 to store the objects based on the requirements: S3 Standard S3 Infrequent Access or Standard-IA S3 Glacier ","date":"2022-05-26","objectID":"/s3/:0:0","tags":["How-to","technology","Amazon S3","aws","ShreyaDhange","VisitingAuthors"],"title":"Amazon S3 at a glance","uri":"/s3/"},{"categories":["How-to","VisitingAuthors","technology"],"content":"S3 Standard Suppose you have hosted a website and its objects are stored in Amazon S3 out of which you only need few data to be fetched frequently. You can use S3 Standard for such objects as its durability is 99.999% and can be fetched in millisec and is highly available. You can store any file that can be as low as KB. You can store the data for an indefinite period of time. ","date":"2022-05-26","objectID":"/s3/:1:0","tags":["How-to","technology","Amazon S3","aws","ShreyaDhange","VisitingAuthors"],"title":"Amazon S3 at a glance","uri":"/s3/"},{"categories":["How-to","VisitingAuthors","technology"],"content":"Key features: Low latency and high performance. Durability is 99.999999999% of objects across multiple Availability Zones Supports SSL for data in transit and encryption of data at rest ","date":"2022-05-26","objectID":"/s3/:1:1","tags":["How-to","technology","Amazon S3","aws","ShreyaDhange","VisitingAuthors"],"title":"Amazon S3 at a glance","uri":"/s3/"},{"categories":["How-to","VisitingAuthors","technology"],"content":"S3 Infrequent Access or Standard-IA: Suppose you have a image gallery of 2020 images and you do not need it frequently but you only need few images as and when needed so you stored such datas in Standard-IA. Standard-IA fetches data at a durability of 99.9% and can store the data uptill 30 days with a low per GB storage price and per GB retrieval charge. ","date":"2022-05-26","objectID":"/s3/:2:0","tags":["How-to","technology","Amazon S3","aws","ShreyaDhange","VisitingAuthors"],"title":"Amazon S3 at a glance","uri":"/s3/"},{"categories":["How-to","VisitingAuthors","technology"],"content":"Key features: Low latency and high performance. Durability is 99.999999999% of objects across multiple Availability Zones Supports SSL for data in transit and encryption of data at rest ","date":"2022-05-26","objectID":"/s3/:2:1","tags":["How-to","technology","Amazon S3","aws","ShreyaDhange","VisitingAuthors"],"title":"Amazon S3 at a glance","uri":"/s3/"},{"categories":["How-to","VisitingAuthors","technology"],"content":"S3 Glacier: S3 Glacier stores archival data that you do not require it on daily bases for example: medical images, news media assets, or genomics data. Its durability of fetching the data is milliseconds to hours to fit your performance needs. You can increase its fetching time by paying certain price per GB pay. It can store 128 KB minimum object size. ","date":"2022-05-26","objectID":"/s3/:3:0","tags":["How-to","technology","Amazon S3","aws","ShreyaDhange","VisitingAuthors"],"title":"Amazon S3 at a glance","uri":"/s3/"},{"categories":["How-to","VisitingAuthors","technology"],"content":"Key features: Low latency and high performance. Data retrieval options from milliseconds to hours as required. Durability is 99.999999999% of objects across multiple Availability Zones Supports SSL for data in transit and encryption of data at rest Data is resilient in the event of the destruction of one entire Availability Zone Shreya Dhange is a Technical Training Developer at Red Hat, who likes to explore and learn new technologies and share her knowledge by writing articles. She has completed her Masters in Computer Science and has gained award for her exemplary academic performance. She has been engaged in creating and delivering content in the cloud and linux space. She can be reached out LinkedIn or via email. ","date":"2022-05-26","objectID":"/s3/:3:1","tags":["How-to","technology","Amazon S3","aws","ShreyaDhange","VisitingAuthors"],"title":"Amazon S3 at a glance","uri":"/s3/"},{"categories":["How-to","VisitingAuthors","technology"],"content":" In this article, we will see what AWS CloudFront is and in what scenario it is used. We will also take a look on some of its advantages and what edge locations are. Let’s consider an example where the user is searching for a website suppose www.primevideo.com and their request and response time should experience least latency. Now, if the amazon server is in the US and the user accessing prime video website is from California so they will experience less latency i.e 2 secs. This is due to less geographical distance between the system generating the request packet and server responding to the request Now if the user is from APAC (say India) and wants to access the same website, they are more likely to experience increased latency, assume 5 secs. To solve this problem Amazon has introduced CloudFront service which works on the principal of CDN (Content Delivery Network). CloudFront leverages the edge locations nearest to your system generating the request packet, which helps in content delivery. Basically it will cache your data and stores the data so that the other users, from the same geographical region as that of primary user, requesting for the same website will have less latency. It is cost effective and data sync is reliable. Edge location will fetch the data faster because it has high bandwidth. Understand the difference that Availability Zones [AZ] are set of data centers that hosts servers, websites, applications, analytics, data processing etc, while Edge Locations [EL] are the data center that primarily are used to cache the data and provide a better user experience by reducing the latency. In simple terms, AWS CloudFront is a fast CDN service that securely delivers data with low latency and high speed transfer of data. You can create CloudFront in any specific region which helps keeping persistent connection with the origin. ","date":"2022-05-26","objectID":"/cloudfront/:0:0","tags":["How-to","technology","aws","aws cloudfront","ShreyaDhange","VisitingAuthors"],"title":"How AWS CloudFront works?","uri":"/cloudfront/"},{"categories":["How-to","VisitingAuthors","technology"],"content":"How Edge location works? As we discussed, edge locations are the data centers that host the web content. Edge location will transfer less requested data to regional edge location. If regional edge location has has deleted it then edge location will request it from the origin. As of writing this article, to deliver content to end users with lower latency, Amazon CloudFront uses a global network of 410+ Points of Presence (400+ Edge locations and 13 regional mid-tier caches) in 90+ cities across 47 countries. ","date":"2022-05-26","objectID":"/cloudfront/:1:0","tags":["How-to","technology","aws","aws cloudfront","ShreyaDhange","VisitingAuthors"],"title":"How AWS CloudFront works?","uri":"/cloudfront/"},{"categories":["How-to","VisitingAuthors","technology"],"content":"Advantages: Reduces the latency It is cost effective. Securely transfers data at fastest speed. High availability and scalability. Shows real time metrics and logging. ","date":"2022-05-26","objectID":"/cloudfront/:2:0","tags":["How-to","technology","aws","aws cloudfront","ShreyaDhange","VisitingAuthors"],"title":"How AWS CloudFront works?","uri":"/cloudfront/"},{"categories":["How-to","VisitingAuthors","technology"],"content":"How AWS leverage CloudFront for its own use? Transfer Acceleration takes advantage of the globally distributed edge locations in Amazon CloudFront. As the data arrives at an edge location, the data is routed to Amazon S3 over an optimized network path. Stay tuned to learn more about its configuration in the next blog Shreya Dhange is a Technical Training Developer at Red Hat, who likes to explore and learn new technologies and share her knowledge by writing articles. She has completed her Masters in Computer Science and has gained award for her exemplary academic performance. She has been engaged in creating and delivering content in the cloud and linux space. She can be reached out LinkedIn or via email. Parth Goswami is an opensource enthusiasts and likes to talk abot AWS and cloud computing. ","date":"2022-05-26","objectID":"/cloudfront/:3:0","tags":["How-to","technology","aws","aws cloudfront","ShreyaDhange","VisitingAuthors"],"title":"How AWS CloudFront works?","uri":"/cloudfront/"},{"categories":["How-to","VisitingAuthors","technology"],"content":"In the previous article, we learned what VPC is, how does it work and what are its benefits. AWS provides a pre-configured to launch our instances, however, the best practice is to create your own custom VPC per your requirements and organization’s network and security policies and launch your instances in the custom VPC. In this article, we will discuss how to configure your own custom VPC. Leaving services running in cloud computing can escalate your bill quickly, hence, it is equally important to understand how to delete the resources once you are done using it. We will also see how to delete the VPC and related resources. ","date":"2022-04-18","objectID":"/configuring_vpc/:0:0","tags":["How-to","technology","aws","VPC","networking","ShreyaDhange","VisitingAuthors"],"title":"Configuring a custom VPC","uri":"/configuring_vpc/"},{"categories":["How-to","VisitingAuthors","technology"],"content":"Steps to configure VPC Go to AWS services from the dashboard in that Network \u0026 Content delivery select VPC. Create a VPC. Every VPC has one default VPC. Note: do not delete the default VPC. Here i have given the name as MyFirstVPC with the CIDR value as 10.0.0.0/16. You can choose any CIDR value of your choice between 10.0.0.0/16 - 10.0.0.0/28. Now VPC is created successfully. Now if you will go the Route Tables on the left side of the dashboard you will see our default route table is been created. To confirm which one is our Route table check the VPC ID. You can give the name to it after confirming. I have given it as MyRouteTable. Inside our VPC we are going to create our subnets i.e. public subnet and private subnet. Give the name as PublicSubnet and PrivateSubnet and the VPC can be chosen from the VPC that you have created. Here i have created it as MyFirstVPC. Availability zone can be chosen any of your choice. CIDR block should be chosen according to the IP range that you have given. Here i give it as 10.0.1.0/24 for PrivateSubnet and 10.0.2.0/24 for PublicSubnet. Now the subnets are been created successfully. As we know all the new entries of the subnets will enter into our default Route table. Go to the Route table in that select Subnet Associates you will see your subnets. Note: Here we don’t have separate Route table all will enter into our default Route table. Now we will create separate Route table in our VPC i.e. Public Route table with the name PublicRT and Private Route table with the name PrivateRT. You can give the name of your choice. Now by default all my subnets are into the default Route table, now we will put our subnets into their respective Route table i.e. PublicSubnet will go to PublicRT and PrivateSubnet will go to PrivateRT. Note: we will not have any duplicate records i.e. 1 subnet can have any 1 Route table. Go to PublicRT in that select Subnet Associations and select Edit subnet associations and select the PublicSubnet and save it. Do the same for PrivateRT. Now if you will go to the the default Route table i.e. MyRouteTable there will be no subnets. Now we will create an Internet Gateway to connect to our VPC. Go to the Internet Gateway on the left side of the dashboard and create it. Here i gave the name as MyGateway you can give of your own choice. Now Internet Gateway is created successfully. Now go to Actions on the right side of your console and Attach the VPC. Now your Internet Gateway is attached to your VPC. Now will attach our Internet Gateway to our public route table which will be accessible through the internet. Now go to Route Tables in that select PublicRT and select Route and select Edit routes. Add Route and give any IP, here i give 0.0.0.0/0 i.e. from anywhere i can access my VPC. Now we will launch our instances into our subnets. Here, we will launch EC2 instance for PublicSubnet. Go to EC2 and select any instance. After selecting go to Configure Instance and select the Network i.e. the VPC you have created. Since we had given my CIDR value as 24 i will get 251 IPs. And we will enable Public IP. After that we will add Security Groups i.e. HTTP and HTTPS. Similarly launch an EC2 instance for PrivateSubnet and the process is same only the change will in Networks, here we will select PrivateSubnet and Public IP will be disabled for this. Now IPs of these instances will be given the range that you had given. Now go and check if my PublicInstance is accessible through the internet and similiarly we will do it for PrivateInstance. If you see that PublicInstance will have public ip wherein PrivateInstance will not have public ip which means only PublicInstance can be accessed through internet. By taking the remote access of the PublicInstance we can check if it is accessible. Since it is accessible, now go and remove the Internet Gateway from the PublicRT and check if it is accessible. PrivateInstance is not accessible through the internet. To access this you will go through the Publi","date":"2022-04-18","objectID":"/configuring_vpc/:1:0","tags":["How-to","technology","aws","VPC","networking","ShreyaDhange","VisitingAuthors"],"title":"Configuring a custom VPC","uri":"/configuring_vpc/"},{"categories":["How-to","VisitingAuthors","technology"],"content":"Steps to delete the VPC Start by terminating the instances. Navigate to Route Tables and remove the entries in the Route Table by going into the Edit routes and remove the NAT Gateway and delete the subnets and then remove public and private route table. Navigate to the Internet Gateway and detach it from VPC. Navigate to the NAT Gateway and delete it. Navigate to the Subnets and remove the entries. Navigate to VPC and delete the VPC. We hope you find this article helpful. Happy learning!👍🏻 Shreya Dhange is a Technical Training Developer at Red Hat, who likes to explore and learn new technologies and share her knowledge by writing articles. She has completed her Masters in Computer Science and has gained award for her exemplary academic performance. She has been engaged in creating and delivering content in the cloud and linux space. She can be reached out LinkedIn or via email. Parth Goswami is an opensource enthusiasts and likes to talk abot AWS and cloud computing. ","date":"2022-04-18","objectID":"/configuring_vpc/:2:0","tags":["How-to","technology","aws","VPC","networking","ShreyaDhange","VisitingAuthors"],"title":"Configuring a custom VPC","uri":"/configuring_vpc/"},{"categories":["How-to","VisitingAuthors","technology"],"content":" If you start from the bare metal layer and go right up until the container layer through virtualization, base OS and the container engine, you would notice that networking runs throughout the stack. Networking is the backbone of any infrastructure. Hence, understanding the networking concepts becomes very important and if we are talking about cloud computing, it becomes ever more important. In this article, I will be discussing how networking works in the cloud computing. For clarity, this article focuses on Virtual Private Cloud[VPC], a networking service provided by Amazon Web Services. Let’s dive into it! ","date":"2022-03-11","objectID":"/decoding_vpc/:0:0","tags":["How-to","technology","aws","VPC","networking","ShreyaDhange","VisitingAuthors"],"title":"How does VPC work?","uri":"/decoding_vpc/"},{"categories":["How-to","VisitingAuthors","technology"],"content":"What is VPC? VPC stands for Virtual Private Cloud and is one of the most fundamental and widely used AWS service which provides a virtual network dedicated for the AWS environments. It is logically isolated from the other virtual networks in the cloud and exists within a single region. VPC allows for more control of AWS Cloud Network with extra layer of security. ","date":"2022-03-11","objectID":"/decoding_vpc/:1:0","tags":["How-to","technology","aws","VPC","networking","ShreyaDhange","VisitingAuthors"],"title":"How does VPC work?","uri":"/decoding_vpc/"},{"categories":["How-to","VisitingAuthors","technology"],"content":"How does VPC works? VPC cannot talk directly to the internet. For that we need Internet Gateway to talk to the internet. Internet Gateway is used to allow resources (subnets) in your VPC to access Internet. VPC can have multiple subnets and these subnets can be public facing or private. Subnets are the the spaces where your applications are running. Subnets are been decided by CIDR( Classless inter-domain routing). Public subnets are the ones that are accessible to the internet and Private subnets are the ones that are not accessible by the internet. For example: Amazon Shopping Website, you cannot access the database of this website but you can access the application of Amazon Shopping Website . This means that the application is running on public subnet and the database is running on private subnet. We decide these public and private subnets using Route Table. Route table is the place where you put all the entries of these public and private subnets i.e. IP address. Every VPC has one default Route table. All subnets that you launch will be a part of this Route table. Everything will go into the Route table i.e. public subnet, private subnet and the internet gateway which means anyone who is coming can access my private subnet which is dangerous. To over come this we use different Route tables i.e. we will make different public route table and different private route table i.e. the internet gateway can access my public subnet and not the private subnet. All the new entries of the subnet will enter into default Route table and whenever you want you can move these entries into public or private subnets. Internet gateway can access the public subnet and public subnet will communicate with the private subnet. If the private subnet wants to talk to the internet but it cannot communicate directly to the internet that time we will use NAT. NAT, Network Address Translation, refers to the proxy server. Private subnet will talk to the internet through NAT but internet will not talk directly to the private subnet, it will first communicate with public subnet and then public subnet will talk to the private subnet. VPC reference diagram ","date":"2022-03-11","objectID":"/decoding_vpc/:2:0","tags":["How-to","technology","aws","VPC","networking","ShreyaDhange","VisitingAuthors"],"title":"How does VPC work?","uri":"/decoding_vpc/"},{"categories":["How-to","VisitingAuthors","technology"],"content":"What is CIDR? CIDR is Classless inter-domain routing. This will decide how many subnets you want to launch in a machine (VPC). Inside these subnets we can launch our machines. It contains the range of IPs i.e. IPv4 and IPv6. IPv6 - 128 bits IPv4 - 32 bits x.x.x.x/16 - x.x.x.x/28 (choose any value between this CIDR range) Suppose, 10.0.0.0/16 = 32-16=16 ==\u003e 2^16= 65536 (65536 IP can be launched in 1 VPC of 16 CIDR value) 10.0.0.0/24 = 32-24= 8 ==\u003e 2^8= 256 10.0.0.0/28= 32-28= 4 ==\u003e 2^4= 16 A minimum 16 subnets and maximum 65536 subnets can be launched in 1 VPC. If your requirements is for more subnets then choose lower CIDR value and if requirement is less then choose higher CIDR value. ","date":"2022-03-11","objectID":"/decoding_vpc/:3:0","tags":["How-to","technology","aws","VPC","networking","ShreyaDhange","VisitingAuthors"],"title":"How does VPC work?","uri":"/decoding_vpc/"},{"categories":["How-to","VisitingAuthors","technology"],"content":"Benefits of VPC Define custom networks. Assigns static private ipv4 addresses to the instances. Define network interfaces and attach one or more network interface to the instance. Define routing between different subnets. Define internet access for the subnets. Define your network security by allowing/denying the traffic. In this article, we discussed what VPC is and how does it work along with some of its benefits. In the next article, I will be discussing how to configure VPC in AWS and the steps to delete it once we are done tinkering with it. Check out how to configure your custom VPC in this article. Shreya Dhange is a Technical Training Developer at Red Hat, who likes to explore and learn new technologies and share her knowledge by writing articles. She has completed her Masters in Computer Science and has gained award for her exemplary academic performance. She has been engaged in creating and delivering content in the cloud and linux space. She can be reached out LinkedIn or via email. Parth Goswami is an opensource enthusiasts and likes to talk abot AWS and cloud computing. ","date":"2022-03-11","objectID":"/decoding_vpc/:4:0","tags":["How-to","technology","aws","VPC","networking","ShreyaDhange","VisitingAuthors"],"title":"How does VPC work?","uri":"/decoding_vpc/"}]